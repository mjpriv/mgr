{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pandas/core/apply.py:285: FutureWarning: A future version of pandas will default to `skipna=True`. To silence this warning, pass `skipna=True|False` explicitly.\n",
      "  values, self.f, axis=self.axis, dummy=dummy, labels=labels\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "path = '../t-sne'\n",
    "os.chdir(path)\n",
    "\n",
    "movies = pd.read_csv('tmdb_movies_data.csv', encoding = 'utf8')\n",
    "movies.dropna(subset=['overview'], inplace=True)\n",
    "movies.apply(lambda x: pd.api.types.infer_dtype(x.values))\n",
    "\n",
    "\n",
    "genres = movies['genres'].str.split('|',expand=True)\n",
    "genres = genres.values\n",
    "genres = pd.DataFrame(genres)\n",
    "genres.columns = ['Genre 1', 'Genre 2', 'Genre 3', 'Genre 4', 'Genre 5']\n",
    "genres = genres.applymap(lambda x: '' if x is None else str(x))\n",
    "\n",
    "# Only 1 genre\n",
    "movies = movies.iloc[(genres.apply(lambda x: sum(x != ''), axis = 1) == 1).values]\n",
    "movies = movies.rename(columns = {'genres':'Genre 1', 'overview':'Description'})\n",
    "\n",
    "# Get rid of Mr. and Mrs. split\n",
    "movies['Description'] = movies['Description'].str.replace('Mrs\\\\.', 'Mrs')\n",
    "movies['Description'] = movies['Description'].str.replace('Mr\\\\.', 'Mr')\n",
    "\n",
    "# More than 1 sentence\n",
    "movies = movies.iloc[((~movies['Description'].str.split('\\\\. ',expand=True).isnull()).apply(sum, axis = 1) > 3).values]\n",
    "\n",
    "# Both genre and description not null\n",
    "movies = movies.iloc[((movies[['Genre 1', 'Description']].isnull()).apply(sum, axis = 1) == 0).values]\n",
    "\n",
    "# Genre share at least 10%\n",
    "movies = movies.set_index('Genre 1').join(pd.DataFrame(movies.groupby('Genre 1').size()/len(movies) > 0.1)).rename(columns = {0:'to_drop'}).reset_index()\n",
    "\n",
    "movies = movies.iloc[movies['to_drop'].values]\n",
    "movies = movies.drop(['to_drop'], axis = 1)\n",
    "\n",
    "# Splitting by sentence\n",
    "sentences = pd.DataFrame(movies['Description'].str.split('\\\\. ',expand=True).unstack()).reset_index().sort_values(['level_1', 'level_0'])\n",
    "sentences = sentences[sentences[0].apply(lambda x: x is not None)]\n",
    "sentences = sentences.set_index('level_1').drop('level_0', axis = 1).rename(columns = {0:'overview'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overview</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level_1</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>After the Ball, a retail fairy tale set in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Kate's dream is to design for couturier houses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Although she is a bright new talent, Kate can'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>No one trusts the daughter of Lee Kassell, a r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Who wants a spy among the sequins and stiletto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>But with the help of a prince of a guy in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>This material was developed and prepared over ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>This special kind of goes back to when he used...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>It felt right to him to shoot this special in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>The show is about an hour long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>The opening act, who is seen at the beginning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>One of his favorite club comics going way back...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Although he's now eighty years old, Claude Lhe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>But his bouts of forgetfulness and confusion a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Even so, he stubbornly refuses to admit that a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Carole, his oldest daughter, wages a daily and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Claude suddenly decides on a whim to go to Flo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>What lies behind this sudden trip?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>When Jay and Annie first got together, their r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>To kick things up a notch, they decide â€“ wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>It seems like a great idea â€“ until they dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>With their reputations on the line, they know...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>but as their race to reclaim their video leads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>A young Peruvian bear with a passion for all t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Finding himself lost and alone at Paddington S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Thank you.') and offer him a temporary haven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>It looks as though his luck has changed until ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>By day, Richard Haig is a successful and well-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>By night, Richard indulges his own romantic f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>But Richard has grown tired of the  game and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>Having lost his earthly body to a trio of S&amp;am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>He soon forces his former mistress to bring hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>but the Cenobites won't be happy about this.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>When Hamilton High's Prom Queen of 1957, Mary ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>Bill Nordham is now the principle of Hamilton ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>However, she is possessed by Mary Lou Maloney ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>Now Bill must face the horror he left behind i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>Heavily pregnant Lenore Davis tells her husban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>They leave their eleven year-old son Chris wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>Lenore feels that something is wrong and deliv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>Lieutenant Perkins comes to the hospital to in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>Frank discovers a dark secret about Lenore and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>Throughout his upbringing unnatural phenomena ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>He is plagued with a terrible curse over which...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>During the full moon he changes into a ferocio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>His love for Christina enables Leon to suppres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>A young coed (Nan Barlow) uses her winter vaca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>Her professor recommends that she spend her ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>He originally cam from that village so he also...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>She gets to the village and notices some weird...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>It seems that the innkeeper is actually the un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>As one of them said when Nan walked away, \"HE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>Three tales are told, each one increasingly te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>The first tells the story of a woman being sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>The second is the story of a man who pays the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>The final tale shows what it is like to see li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>In the heart of Styria the Karnstein Family, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>Baron Hartog whose family are all victims of K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>One grave he cannot locate is that of the lege...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>Years of peace follow that grisly night until ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1893 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  overview\n",
       "level_1                                                   \n",
       "23       After the Ball, a retail fairy tale set in the...\n",
       "23          Kate's dream is to design for couturier houses\n",
       "23       Although she is a bright new talent, Kate can'...\n",
       "23       No one trusts the daughter of Lee Kassell, a r...\n",
       "23       Who wants a spy among the sequins and stiletto...\n",
       "...                                                    ...\n",
       "412      The final tale shows what it is like to see li...\n",
       "413      In the heart of Styria the Karnstein Family, e...\n",
       "413      Baron Hartog whose family are all victims of K...\n",
       "413      One grave he cannot locate is that of the lege...\n",
       "413      Years of peace follow that grisly night until ...\n",
       "\n",
       "[1893 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting by sentence\n",
    "sentences = pd.DataFrame(movies['Description'].str.split('\\\\. ',expand=True).unstack()).reset_index().sort_values(['level_1', 'level_0'])\n",
    "sentences = sentences[sentences[0].apply(lambda x: x is not None)]\n",
    "sentences = sentences.set_index('level_1').drop('level_0', axis = 1).rename(columns = {0:'overview'})\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mj_dtm(description):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    import string\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "    stop = stopwords.words('english')\n",
    "\n",
    "    docs = description\n",
    "\n",
    "    docs = docs.apply(lambda x: x.translate(str.maketrans({key: None for key in string.punctuation})))\n",
    "    docs = docs.apply(lambda x: x.lower())\n",
    "\n",
    "    docs = docs.apply(lambda x: [x for x in x.split() if x not in stop])\n",
    "    docs = docs.apply(lambda x: ' '.join(x))\n",
    "\n",
    "    lancaster = LancasterStemmer()\n",
    "\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # docs = [lancaster.stem(word) for word in docs]\n",
    "\n",
    "    docs = [wordnet_lemmatizer.lemmatize(word) for word in docs]\n",
    "\n",
    "    vec = CountVectorizer()\n",
    "\n",
    "    X = vec.fit_transform(docs)\n",
    "    df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "\n",
    "    # Words occurred in more than 1 movie\n",
    "    df = df.iloc[:,((df>0).apply(sum) > 1).values]\n",
    "    import re\n",
    "    df = df.iloc[:, pd.Series(df.columns).apply(lambda x: re.match(\"^[0-9]\", x) is None).values]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = mj_dtm(movies['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test = mj_dtm(sentences['overview'])\n",
    "df_test = df_test.set_index(sentences.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['broken',\n",
       " 'building',\n",
       " 'buried',\n",
       " 'chance',\n",
       " 'changed',\n",
       " 'childhood',\n",
       " 'chronicles',\n",
       " 'community',\n",
       " 'computer',\n",
       " 'craig',\n",
       " 'creative',\n",
       " 'daily',\n",
       " 'darkness',\n",
       " 'desire',\n",
       " 'detective',\n",
       " 'determined',\n",
       " 'dont',\n",
       " 'drugs',\n",
       " 'du',\n",
       " 'economic',\n",
       " 'else',\n",
       " 'entire',\n",
       " 'familys',\n",
       " 'far',\n",
       " 'fashion',\n",
       " 'fast',\n",
       " 'feelings',\n",
       " 'finding',\n",
       " 'francis',\n",
       " 'french',\n",
       " 'front',\n",
       " 'george',\n",
       " 'got',\n",
       " 'grown',\n",
       " 'half',\n",
       " 'happy',\n",
       " 'hero',\n",
       " 'hospital',\n",
       " 'hot',\n",
       " 'hour',\n",
       " 'ice',\n",
       " 'incredible',\n",
       " 'india',\n",
       " 'intimate',\n",
       " 'involved',\n",
       " 'jane',\n",
       " 'joins',\n",
       " 'joy',\n",
       " 'lead',\n",
       " 'legend',\n",
       " 'los',\n",
       " 'loves',\n",
       " 'major',\n",
       " 'martial',\n",
       " 'mary',\n",
       " 'match',\n",
       " 'matt',\n",
       " 'megan',\n",
       " 'middle',\n",
       " 'missing',\n",
       " 'modern',\n",
       " 'mom',\n",
       " 'moment',\n",
       " 'moon',\n",
       " 'nazi',\n",
       " 'near',\n",
       " 'nobody',\n",
       " 'oscar',\n",
       " 'outside',\n",
       " 'owner',\n",
       " 'part',\n",
       " 'photos',\n",
       " 'pictures',\n",
       " 'plays',\n",
       " 'popular',\n",
       " 'power',\n",
       " 'private',\n",
       " 'professional',\n",
       " 'quite',\n",
       " 'radio',\n",
       " 'release',\n",
       " 'returning',\n",
       " 'revolution',\n",
       " 'rose',\n",
       " 'safe',\n",
       " 'scott',\n",
       " 'sea',\n",
       " 'secrets',\n",
       " 'sees',\n",
       " 'shocking',\n",
       " 'single',\n",
       " 'slowly',\n",
       " 'someone',\n",
       " 'spend',\n",
       " 'spirit',\n",
       " 'stand',\n",
       " 'stop',\n",
       " 'struggle',\n",
       " 'struggles',\n",
       " 'sudden',\n",
       " 'support',\n",
       " 'terrible',\n",
       " 'thing',\n",
       " 'third',\n",
       " 'toward',\n",
       " 'understand',\n",
       " 'unique',\n",
       " 'used',\n",
       " 'vacation',\n",
       " 'victim',\n",
       " 'violent',\n",
       " 'ways',\n",
       " 'weekend',\n",
       " 'weeks',\n",
       " 'whilst',\n",
       " 'whole',\n",
       " 'winning',\n",
       " 'worker',\n",
       " 'written',\n",
       " 'able',\n",
       " 'accidentally',\n",
       " 'actress',\n",
       " 'adults',\n",
       " 'adventure',\n",
       " 'agent',\n",
       " 'agrees',\n",
       " 'airline',\n",
       " 'allen',\n",
       " 'amanda',\n",
       " 'anything',\n",
       " 'arrives',\n",
       " 'attitude',\n",
       " 'bank',\n",
       " 'becoming',\n",
       " 'beginning',\n",
       " 'behaviour',\n",
       " 'beloved',\n",
       " 'billy',\n",
       " 'blonde',\n",
       " 'brand',\n",
       " 'build',\n",
       " 'butler',\n",
       " 'came',\n",
       " 'campers',\n",
       " 'captured',\n",
       " 'carol',\n",
       " 'case',\n",
       " 'cemetery',\n",
       " 'center',\n",
       " 'century',\n",
       " 'character',\n",
       " 'charming',\n",
       " 'christian',\n",
       " 'club',\n",
       " 'complete',\n",
       " 'constant',\n",
       " 'contemporary',\n",
       " 'countryside',\n",
       " 'courage',\n",
       " 'covers',\n",
       " 'crew',\n",
       " 'dangerous',\n",
       " 'daniel',\n",
       " 'develops',\n",
       " 'didnt',\n",
       " 'die',\n",
       " 'died',\n",
       " 'dies',\n",
       " 'discovered',\n",
       " 'discovery',\n",
       " 'easy',\n",
       " 'edge',\n",
       " 'embark',\n",
       " 'emotional',\n",
       " 'en',\n",
       " 'enter',\n",
       " 'epic',\n",
       " 'especially',\n",
       " 'eve',\n",
       " 'evidence',\n",
       " 'exactly',\n",
       " 'except',\n",
       " 'explores',\n",
       " 'eye',\n",
       " 'features',\n",
       " 'flight',\n",
       " 'food',\n",
       " 'forever',\n",
       " 'forward',\n",
       " 'free',\n",
       " 'fresh',\n",
       " 'friendship',\n",
       " 'funeral',\n",
       " 'gay',\n",
       " 'generation',\n",
       " 'given',\n",
       " 'god',\n",
       " 'government',\n",
       " 'grand',\n",
       " 'happen',\n",
       " 'happens',\n",
       " 'heads',\n",
       " 'hearts',\n",
       " 'held',\n",
       " 'hell',\n",
       " 'heâ',\n",
       " 'hollywood',\n",
       " 'hoping',\n",
       " 'horrific',\n",
       " 'hungry',\n",
       " 'husbands',\n",
       " 'innocent',\n",
       " 'inside',\n",
       " 'interview',\n",
       " 'issues',\n",
       " 'james',\n",
       " 'jason',\n",
       " 'jay',\n",
       " 'jimmy',\n",
       " 'jon',\n",
       " 'jones',\n",
       " 'journalist',\n",
       " 'julian',\n",
       " 'killer',\n",
       " 'lady',\n",
       " 'land',\n",
       " 'latest',\n",
       " 'learn',\n",
       " 'lonely',\n",
       " 'lose',\n",
       " 'loss',\n",
       " 'loved',\n",
       " 'master',\n",
       " 'material',\n",
       " 'michigan',\n",
       " 'months',\n",
       " 'movies',\n",
       " 'murders',\n",
       " 'musical',\n",
       " 'mustang',\n",
       " 'novel',\n",
       " 'observed',\n",
       " 'odds',\n",
       " 'offer',\n",
       " 'officer',\n",
       " 'often',\n",
       " 'open',\n",
       " 'pair',\n",
       " 'paris',\n",
       " 'paths',\n",
       " 'peace',\n",
       " 'perhaps',\n",
       " 'pierre',\n",
       " 'plenty',\n",
       " 'poor',\n",
       " 'pressure',\n",
       " 'process',\n",
       " 'producers',\n",
       " 'prom',\n",
       " 'promised',\n",
       " 'promises',\n",
       " 'rare',\n",
       " 'rather',\n",
       " 'recently',\n",
       " 'red',\n",
       " 'remote',\n",
       " 'research',\n",
       " 'restaurant',\n",
       " 'revealed',\n",
       " 'revealing',\n",
       " 'rise',\n",
       " 'role',\n",
       " 'room',\n",
       " 'running',\n",
       " 'said',\n",
       " 'sanity',\n",
       " 'science',\n",
       " 'seek',\n",
       " 'send',\n",
       " 'setting',\n",
       " 'share',\n",
       " 'shell',\n",
       " 'sons',\n",
       " 'st',\n",
       " 'stage',\n",
       " 'started',\n",
       " 'steal',\n",
       " 'stunts',\n",
       " 'style',\n",
       " 'suffering',\n",
       " 'surprising',\n",
       " 'teen',\n",
       " 'theres',\n",
       " 'thrown',\n",
       " 'tim',\n",
       " 'today',\n",
       " 'travel',\n",
       " 'unfortunately',\n",
       " 'unknown',\n",
       " 'using',\n",
       " 'various',\n",
       " 'violence',\n",
       " 'visit',\n",
       " 'waiting',\n",
       " 'wealthy',\n",
       " 'west',\n",
       " 'writing',\n",
       " 'younger',\n",
       " 'youngest',\n",
       " 'academy',\n",
       " 'actors',\n",
       " 'actually',\n",
       " 'affairs',\n",
       " 'aggressive',\n",
       " 'aging',\n",
       " 'almost',\n",
       " 'already',\n",
       " 'alternative',\n",
       " 'ambitious',\n",
       " 'amp',\n",
       " 'animals',\n",
       " 'anniversary',\n",
       " 'anyone',\n",
       " 'arrest',\n",
       " 'arrive',\n",
       " 'asked',\n",
       " 'aspect',\n",
       " 'audience',\n",
       " 'avoid',\n",
       " 'awful',\n",
       " 'baker',\n",
       " 'bar',\n",
       " 'bee',\n",
       " 'beer',\n",
       " 'beethoven',\n",
       " 'beneath',\n",
       " 'blog',\n",
       " 'blue',\n",
       " 'bond',\n",
       " 'books',\n",
       " 'bride',\n",
       " 'brilliant',\n",
       " 'brutal',\n",
       " 'built',\n",
       " 'businessman',\n",
       " 'campus',\n",
       " 'care',\n",
       " 'caused',\n",
       " 'celebrate',\n",
       " 'celebrated',\n",
       " 'chief',\n",
       " 'childrens',\n",
       " 'class',\n",
       " 'claude',\n",
       " 'clear',\n",
       " 'coach',\n",
       " 'competition',\n",
       " 'competitors',\n",
       " 'condition',\n",
       " 'confused',\n",
       " 'constantly',\n",
       " 'cool',\n",
       " 'crazy',\n",
       " 'cross',\n",
       " 'dana',\n",
       " 'dance',\n",
       " 'date',\n",
       " 'deal',\n",
       " 'debut',\n",
       " 'decision',\n",
       " 'desires',\n",
       " 'desperately',\n",
       " 'destroy',\n",
       " 'diagnosed',\n",
       " 'directorial',\n",
       " 'disorder',\n",
       " 'distant',\n",
       " 'done',\n",
       " 'door',\n",
       " 'drinking',\n",
       " 'due',\n",
       " 'earth',\n",
       " 'eating',\n",
       " 'eccentric',\n",
       " 'effort',\n",
       " 'eight',\n",
       " 'elena',\n",
       " 'elite',\n",
       " 'ellis',\n",
       " 'encounters',\n",
       " 'ended',\n",
       " 'ends',\n",
       " 'enough',\n",
       " 'event',\n",
       " 'experiences',\n",
       " 'exploration',\n",
       " 'explore',\n",
       " 'explosive',\n",
       " 'exposes',\n",
       " 'eyes',\n",
       " 'facing',\n",
       " 'fact',\n",
       " 'factory',\n",
       " 'fail',\n",
       " 'falling',\n",
       " 'fame',\n",
       " 'fastest',\n",
       " 'favorite',\n",
       " 'female',\n",
       " 'festival',\n",
       " 'fights',\n",
       " 'filmed',\n",
       " 'fire',\n",
       " 'fix',\n",
       " 'floor',\n",
       " 'follow',\n",
       " 'followed',\n",
       " 'forbidden',\n",
       " 'france',\n",
       " 'frank',\n",
       " 'freedom',\n",
       " 'frustrated',\n",
       " 'fun',\n",
       " 'global',\n",
       " 'gone',\n",
       " 'gorgeous',\n",
       " 'grace',\n",
       " 'graduate',\n",
       " 'guilt',\n",
       " 'gun',\n",
       " 'guy',\n",
       " 'happily',\n",
       " 'haunted',\n",
       " 'helen',\n",
       " 'hires',\n",
       " 'holds',\n",
       " 'homeless',\n",
       " 'hours',\n",
       " 'houses',\n",
       " 'humor',\n",
       " 'iconic',\n",
       " 'ideal',\n",
       " 'identity',\n",
       " 'illness',\n",
       " 'increasingly',\n",
       " 'influence',\n",
       " 'institution',\n",
       " 'intensely',\n",
       " 'international',\n",
       " 'internet',\n",
       " 'invites',\n",
       " 'involving',\n",
       " 'isnt',\n",
       " 'italian',\n",
       " 'italy',\n",
       " 'join',\n",
       " 'jr',\n",
       " 'jungle',\n",
       " 'kennedy',\n",
       " 'kill',\n",
       " 'knowledge',\n",
       " 'lauren',\n",
       " 'league',\n",
       " 'led',\n",
       " 'letter',\n",
       " 'lie',\n",
       " 'light',\n",
       " 'line',\n",
       " 'lola',\n",
       " 'lucky',\n",
       " 'lucy',\n",
       " 'luxury',\n",
       " 'machine',\n",
       " 'male',\n",
       " 'manage',\n",
       " 'manages',\n",
       " 'marc',\n",
       " 'marcus',\n",
       " 'marry',\n",
       " 'mass',\n",
       " 'max',\n",
       " 'mechanic',\n",
       " 'media',\n",
       " 'medical',\n",
       " 'meeting',\n",
       " 'members',\n",
       " 'memories',\n",
       " 'mental',\n",
       " 'miles',\n",
       " 'minister',\n",
       " 'mistaken',\n",
       " 'morning',\n",
       " 'moved',\n",
       " 'murderous',\n",
       " 'nature',\n",
       " 'navigate',\n",
       " 'needs',\n",
       " 'neighbors',\n",
       " 'news',\n",
       " 'nightmare',\n",
       " 'nights',\n",
       " 'november',\n",
       " 'official',\n",
       " 'opens',\n",
       " 'overbearing',\n",
       " 'pacific',\n",
       " 'pain',\n",
       " 'partner',\n",
       " 'path',\n",
       " 'performed',\n",
       " 'period',\n",
       " 'piece',\n",
       " 'pioneer',\n",
       " 'political',\n",
       " 'possible',\n",
       " 'principal',\n",
       " 'produce',\n",
       " 'project',\n",
       " 'prostitute',\n",
       " 'protect',\n",
       " 'proves',\n",
       " 'provide',\n",
       " 'psychiatrist',\n",
       " 'queen',\n",
       " 'rachel',\n",
       " 'realizes',\n",
       " 'reason',\n",
       " 'relationships',\n",
       " 'released',\n",
       " 'religious',\n",
       " 'responsible',\n",
       " 'rome',\n",
       " 'russia',\n",
       " 'satan',\n",
       " 'score',\n",
       " 'screen',\n",
       " 'secretly',\n",
       " 'sharon',\n",
       " 'shoot',\n",
       " 'shy',\n",
       " 'side',\n",
       " 'simon',\n",
       " 'site',\n",
       " 'southern',\n",
       " 'store',\n",
       " 'strength',\n",
       " 'strikes',\n",
       " 'stuck',\n",
       " 'studios',\n",
       " 'suffers',\n",
       " 'suicide',\n",
       " 'sure',\n",
       " 'taken',\n",
       " 'talking',\n",
       " 'teams',\n",
       " 'therapy',\n",
       " 'thinking',\n",
       " 'thousands',\n",
       " 'threatens',\n",
       " 'tiny',\n",
       " 'title',\n",
       " 'tom',\n",
       " 'took',\n",
       " 'tour',\n",
       " 'tow',\n",
       " 'track',\n",
       " 'trapped',\n",
       " 'traveling',\n",
       " 'tree',\n",
       " 'unfolds',\n",
       " 'urban',\n",
       " 'use',\n",
       " 'vincent',\n",
       " 'waitress',\n",
       " 'walt',\n",
       " 'washington',\n",
       " 'watching',\n",
       " 'wikileaks',\n",
       " 'winner',\n",
       " 'wins',\n",
       " 'wont',\n",
       " 'woods',\n",
       " 'woody',\n",
       " 'word',\n",
       " 'worse',\n",
       " 'worth',\n",
       " 'yearold',\n",
       " 'abuse',\n",
       " 'activity',\n",
       " 'acts',\n",
       " 'addict',\n",
       " 'addition',\n",
       " 'adult',\n",
       " 'adulthood',\n",
       " 'afraid',\n",
       " 'al',\n",
       " 'allens',\n",
       " 'ambition',\n",
       " 'americans',\n",
       " 'amy',\n",
       " 'angry',\n",
       " 'annabelle',\n",
       " 'answers',\n",
       " 'appear',\n",
       " 'approaches',\n",
       " 'area',\n",
       " 'arent',\n",
       " 'assange',\n",
       " 'assigned',\n",
       " 'athletic',\n",
       " 'attack',\n",
       " 'attend',\n",
       " 'attract',\n",
       " 'attracted',\n",
       " 'audiences',\n",
       " 'author',\n",
       " 'available',\n",
       " 'award',\n",
       " 'awardwinning',\n",
       " 'babysitter',\n",
       " 'backdrop',\n",
       " 'battles',\n",
       " 'bear',\n",
       " 'beast',\n",
       " 'beaten',\n",
       " 'bedroom',\n",
       " 'befriends',\n",
       " 'behindthescenes',\n",
       " 'benjamin',\n",
       " 'bernard',\n",
       " 'billionaire',\n",
       " 'birth',\n",
       " 'bizarre',\n",
       " 'blind',\n",
       " 'bound',\n",
       " 'broadway',\n",
       " 'buddies',\n",
       " 'budget',\n",
       " 'buy',\n",
       " 'california',\n",
       " 'call',\n",
       " 'calls',\n",
       " 'camper',\n",
       " 'canâ',\n",
       " 'captures',\n",
       " 'careers',\n",
       " 'catch',\n",
       " 'catches',\n",
       " 'celebration',\n",
       " 'centers',\n",
       " 'certain',\n",
       " 'champion',\n",
       " 'characters',\n",
       " 'charged',\n",
       " 'charlotte',\n",
       " 'choice',\n",
       " 'chosen',\n",
       " 'claim',\n",
       " 'close',\n",
       " 'closer',\n",
       " 'clothes',\n",
       " 'cocaine',\n",
       " 'cold',\n",
       " 'common',\n",
       " 'complex',\n",
       " 'conflict',\n",
       " 'confronted',\n",
       " 'connection',\n",
       " 'conversation',\n",
       " 'convicted',\n",
       " 'cops',\n",
       " 'corporate',\n",
       " 'cost',\n",
       " 'countless',\n",
       " 'countries',\n",
       " 'court',\n",
       " 'cousin',\n",
       " 'craft',\n",
       " 'crash',\n",
       " 'cream',\n",
       " 'creating',\n",
       " 'creator',\n",
       " 'creature',\n",
       " 'credit',\n",
       " 'cruel',\n",
       " 'culture',\n",
       " 'cut',\n",
       " 'cycling',\n",
       " 'dan',\n",
       " 'dc',\n",
       " 'deaths',\n",
       " 'debate',\n",
       " 'deeply',\n",
       " 'definitive',\n",
       " 'demise',\n",
       " 'denial',\n",
       " 'depression',\n",
       " 'desert',\n",
       " 'destruction',\n",
       " 'develop',\n",
       " 'devoted',\n",
       " 'diane',\n",
       " 'divorce',\n",
       " 'divorced',\n",
       " 'doesnâ',\n",
       " 'dollar',\n",
       " 'dollars',\n",
       " 'donald',\n",
       " 'dress',\n",
       " 'drink',\n",
       " 'driver',\n",
       " 'dropped',\n",
       " 'efforts',\n",
       " 'elections',\n",
       " 'eleven',\n",
       " 'elsewhere',\n",
       " 'embarrassment',\n",
       " 'employee',\n",
       " 'endless',\n",
       " 'energy',\n",
       " 'engineer',\n",
       " 'enjoy',\n",
       " 'enlists',\n",
       " 'escaping',\n",
       " 'estranged',\n",
       " 'everyday',\n",
       " 'experience',\n",
       " 'explain',\n",
       " 'extra',\n",
       " 'failed',\n",
       " 'families',\n",
       " 'fateful',\n",
       " 'fighter',\n",
       " 'fighting',\n",
       " 'filming',\n",
       " 'filmmakers',\n",
       " 'firm',\n",
       " 'flat',\n",
       " 'fleet',\n",
       " 'florida',\n",
       " 'focused',\n",
       " 'focuses',\n",
       " 'foreign',\n",
       " 'forget',\n",
       " 'fort',\n",
       " 'fourth',\n",
       " 'fred',\n",
       " 'frequent',\n",
       " 'frozen',\n",
       " 'fulfill',\n",
       " 'furniture',\n",
       " 'garage',\n",
       " 'ghosts',\n",
       " 'grandmother',\n",
       " 'grandmothers',\n",
       " 'ground',\n",
       " 'groundbreaking',\n",
       " 'guide',\n",
       " 'guru',\n",
       " 'hair',\n",
       " 'halloween',\n",
       " 'hamilton',\n",
       " 'hands',\n",
       " 'happened',\n",
       " 'health',\n",
       " 'heather',\n",
       " 'highly',\n",
       " 'hills',\n",
       " 'hitchcock',\n",
       " 'holiday',\n",
       " 'hometown',\n",
       " 'horrifying',\n",
       " 'howard',\n",
       " 'human',\n",
       " 'humanity',\n",
       " 'ii',\n",
       " 'images',\n",
       " 'impact',\n",
       " 'include',\n",
       " 'includes',\n",
       " 'infamous',\n",
       " 'influenced',\n",
       " 'inspector',\n",
       " 'institutions',\n",
       " 'interested',\n",
       " 'interesting',\n",
       " 'intertwined',\n",
       " 'irene',\n",
       " 'isnâ',\n",
       " 'jennifer',\n",
       " 'jessica',\n",
       " 'jesus',\n",
       " 'johnny',\n",
       " 'joining',\n",
       " 'katherine',\n",
       " 'knew',\n",
       " 'ladies',\n",
       " 'lake',\n",
       " 'lance',\n",
       " 'laughter',\n",
       " 'laura',\n",
       " 'leader',\n",
       " 'leaders',\n",
       " 'length',\n",
       " 'less',\n",
       " 'lets',\n",
       " 'lines',\n",
       " 'liz',\n",
       " 'lot',\n",
       " 'lou',\n",
       " 'lying',\n",
       " 'main',\n",
       " 'maria',\n",
       " 'marianne',\n",
       " 'mates',\n",
       " 'mean',\n",
       " 'meanwhile',\n",
       " 'member',\n",
       " 'met',\n",
       " 'minds',\n",
       " 'mission',\n",
       " 'modest',\n",
       " 'motorcycle',\n",
       " 'mountain',\n",
       " 'movement',\n",
       " 'moves',\n",
       " 'murderer',\n",
       " 'musicians',\n",
       " 'mutual',\n",
       " 'mystery',\n",
       " 'namely',\n",
       " 'nancy',\n",
       " 'native',\n",
       " 'neighbor',\n",
       " 'neither',\n",
       " 'north',\n",
       " 'nowhere',\n",
       " 'nurse',\n",
       " 'obsessed',\n",
       " 'obvious',\n",
       " 'odd',\n",
       " 'offers',\n",
       " 'officers',\n",
       " 'oldest',\n",
       " 'olivier',\n",
       " 'opening',\n",
       " 'owners',\n",
       " 'partners',\n",
       " 'pass',\n",
       " 'passion',\n",
       " 'patient',\n",
       " 'patrick',\n",
       " 'pay',\n",
       " 'performance',\n",
       " 'peter',\n",
       " 'pets',\n",
       " 'photographer',\n",
       " 'picture',\n",
       " 'plagued',\n",
       " 'plane',\n",
       " 'planned',\n",
       " 'playing',\n",
       " 'point',\n",
       " 'portrays',\n",
       " 'potential',\n",
       " 'prep',\n",
       " 'prepared',\n",
       " 'presence',\n",
       " 'presented',\n",
       " 'presents',\n",
       " 'previous',\n",
       " 'privileged',\n",
       " 'profound',\n",
       " 'progress',\n",
       " 'promising',\n",
       " 'proposal',\n",
       " 'punish',\n",
       " 'puppet',\n",
       " 'quiet',\n",
       " 'racing',\n",
       " 'raise',\n",
       " 'reading',\n",
       " 'receives',\n",
       " 'recent',\n",
       " 'reclaim',\n",
       " 'recorded',\n",
       " 'redefine',\n",
       " 'refuse',\n",
       " 'regardless',\n",
       " 'regime',\n",
       " 'reluctant',\n",
       " 'remake',\n",
       " 'rescue',\n",
       " 'rescues',\n",
       " 'retail',\n",
       " 'retirement',\n",
       " 'returned',\n",
       " 'ride',\n",
       " 'rock',\n",
       " 'romance',\n",
       " 'romantic',\n",
       " 'rudy',\n",
       " 'rural',\n",
       " 'sacrifice',\n",
       " 'saw',\n",
       " 'says',\n",
       " 'scandals',\n",
       " 'schools',\n",
       " 'script',\n",
       " 'searches',\n",
       " 'season',\n",
       " 'security',\n",
       " 'seeing',\n",
       " 'seeking',\n",
       " 'seeks',\n",
       " 'self',\n",
       " 'selling',\n",
       " 'sends',\n",
       " 'separate',\n",
       " 'shane',\n",
       " 'shared',\n",
       " 'shares',\n",
       " 'shiny',\n",
       " 'shooting',\n",
       " 'showman',\n",
       " 'singer',\n",
       " 'sir',\n",
       " 'smalltown',\n",
       " 'smart',\n",
       " 'smash',\n",
       " 'smith',\n",
       " 'society',\n",
       " 'solitude',\n",
       " 'spanish',\n",
       " 'speaks',\n",
       " 'spelling',\n",
       " 'spends',\n",
       " 'spreading',\n",
       " 'starring',\n",
       " 'stewart',\n",
       " 'storm',\n",
       " 'stranger',\n",
       " 'streets',\n",
       " 'strong',\n",
       " 'study',\n",
       " 'stunning',\n",
       " 'subject',\n",
       " 'subjects',\n",
       " 'suburban',\n",
       " 'sunshine',\n",
       " 'survive',\n",
       " 'sweet',\n",
       " 'teachers',\n",
       " 'teenagers',\n",
       " 'telling',\n",
       " 'ten',\n",
       " 'test',\n",
       " 'thanksgiving',\n",
       " 'theyve',\n",
       " 'thirty',\n",
       " 'thoughts',\n",
       " 'threat',\n",
       " 'thriller',\n",
       " 'tokyo',\n",
       " 'tommy',\n",
       " 'torino',\n",
       " 'tormented',\n",
       " 'torn',\n",
       " 'torture',\n",
       " 'transformation',\n",
       " 'treasure',\n",
       " 'treatment',\n",
       " 'trio',\n",
       " 'truck',\n",
       " 'truly',\n",
       " 'turning',\n",
       " 'typical',\n",
       " 'unable',\n",
       " 'unconventional',\n",
       " 'uncover',\n",
       " 'understanding',\n",
       " 'unexpectedly',\n",
       " 'unspeakable',\n",
       " 'unusual',\n",
       " 'vengeance',\n",
       " 'visions',\n",
       " 'warner',\n",
       " 'wealth',\n",
       " 'wearing',\n",
       " 'went',\n",
       " 'whats',\n",
       " 'whos',\n",
       " 'william',\n",
       " 'willing',\n",
       " 'window',\n",
       " 'wish',\n",
       " 'wishes',\n",
       " 'wit',\n",
       " 'wonder',\n",
       " 'words',\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm = movies['Genre 1'].reset_index()[movies['Genre 1'].reset_index().columns.difference(['index'])]\n",
    "groups_filter = pd.concat([dtm, df], axis=1)\n",
    "groups_filter = groups_filter.groupby('Genre 1').sum()[df.columns]\n",
    "groups_filter = groups_filter.T\n",
    "groups_filter['n_words'] = groups_filter.apply(sum, axis = 1)\n",
    "\n",
    "# Taking into consideration n_words which is always > 0\n",
    "groups_filter['is_common'] = groups_filter.apply(lambda x: 1 if sum(x > 0) > 2 else 0, axis = 1)\n",
    "\n",
    "groups_filter.sort_values(['is_common', 'n_words'], ascending = False)\n",
    "\n",
    "# Get rid of top 20% words which are common (?)\n",
    "groups_filter = groups_filter.sort_values(['is_common', 'n_words'], ascending = False).iloc[int(np.floor(groups_filter.shape[0]/5)):groups_filter.shape[0]]\n",
    "\n",
    "word_list = list(groups_filter.index)\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Genre 1\n",
       "Comedy         1213\n",
       "Documentary     992\n",
       "Drama          1444\n",
       "Horror          706\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups_filter.iloc[:, 0:groups_filter.shape[1]-2]\n",
    "groups_filter = groups_filter.iloc[:, 0:groups_filter.shape[1]-2]\n",
    "n_filter = (groups_filter > 0).sum()\n",
    "n_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Genre 1</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Horror</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abandoned</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accepts</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accidentally</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accounts</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actors</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actress</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actually</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adams</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adulthood</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adults</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adventure</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>affairs</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>afraid</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>africanamerican</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aggressive</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aging</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agonizing</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agrees</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airline</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>al</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alexandra</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allen</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allens</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>already</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amanda</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>americas</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amp</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amy</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ancient</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>angela</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>violence</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>violent</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>violently</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visit</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waitress</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walt</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>watching</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ways</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wealthy</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>week</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekend</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weeks</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>werewolf</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>west</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whilst</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whole</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>widow</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wikileaks</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>winner</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>winning</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>witch</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>witches</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woods</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woody</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worker</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worldwide</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>writing</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>younger</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youngest</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youth</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>763 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Genre 1       Comedy  Documentary  Drama  Horror\n",
       "abandoned        NaN          NaN    NaN     6.0\n",
       "accepts          2.0          NaN    NaN     NaN\n",
       "accidentally     NaN          NaN    NaN     2.0\n",
       "accounts         NaN          2.0    NaN     NaN\n",
       "actors           NaN          3.0    NaN     NaN\n",
       "...              ...          ...    ...     ...\n",
       "worldwide        NaN          2.0    NaN     NaN\n",
       "writing          NaN          3.0    NaN     NaN\n",
       "younger          NaN          2.0    NaN     NaN\n",
       "youngest         NaN          NaN    4.0     NaN\n",
       "youth            NaN          NaN    3.0     NaN\n",
       "\n",
       "[763 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups_filter = groups_filter.T\n",
    "\n",
    "def sorted(s, num):\n",
    "    tmp = s.sort_values(ascending=False)[:num]  # earlier s.order(..)\n",
    "#     tmp.index = range(num)\n",
    "    return tmp\n",
    "\n",
    "groups_filter = groups_filter[word_list]\n",
    "groups_filter\n",
    "sorted_words = groups_filter.T.apply(lambda x: sorted(x, 200))\n",
    "sorted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Each row now has 4 intersections with all 4 genres\n",
    "# df_array = df_test[list(sorted_words.index)].values\n",
    "# tfm = [row * (~sorted_words.isnull().T) for row in df_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "comedy_base = ~sorted_words['Comedy'][~sorted_words.isnull()['Comedy'].T].isnull()\n",
    "horror_base = ~sorted_words['Horror'][~sorted_words.isnull()['Horror'].T].isnull()\n",
    "documentary_base = ~sorted_words['Documentary'][~sorted_words.isnull()['Documentary'].T].isnull()\n",
    "drama_base = ~sorted_words['Drama'][~sorted_words.isnull()['Drama'].T].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comedy_test = df_test[comedy_base.index.values]\n",
    "horror_test = df_test[horror_base.index.values]\n",
    "documentary_test = df_test[documentary_base.index.values]\n",
    "drama_test = df_test[drama_base.index.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <span>StrVector with 9 elements.</span>\n",
       "        <table>\n",
       "        <tbody>\n",
       "          <tr>\n",
       "          \n",
       "            <td>\n",
       "            'TDA'\n",
       "            </td>\n",
       "          \n",
       "            <td>\n",
       "            'tools'\n",
       "            </td>\n",
       "          \n",
       "            <td>\n",
       "            'stats'\n",
       "            </td>\n",
       "          \n",
       "            <td>\n",
       "            ...\n",
       "            </td>\n",
       "          \n",
       "            <td>\n",
       "            'datasets'\n",
       "            </td>\n",
       "          \n",
       "            <td>\n",
       "            'methods'\n",
       "            </td>\n",
       "          \n",
       "            <td>\n",
       "            'base'\n",
       "            </td>\n",
       "          \n",
       "          </tr>\n",
       "        </tbody>\n",
       "        </table>\n",
       "        "
      ],
      "text/plain": [
       "R object with classes: ('character',) mapped to:\n",
       "['TDA', 'tools', 'stats', 'graphics', ..., 'utils', 'datasets', 'methods', 'base']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy as sp\n",
    "from rpy2.robjects.packages import importr\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import r\n",
    "# import pandas.rpy.common as com\n",
    "\n",
    "# import R's \"base\" package\n",
    "base = importr('base')\n",
    "\n",
    "# import R's \"utils\" package\n",
    "utils = importr('utils')\n",
    "\n",
    "# import rpy2's package module\n",
    "import rpy2.robjects.packages as rpackages\n",
    "\n",
    "# import R's utility package\n",
    "utils = rpackages.importr('utils')\n",
    "\n",
    "# select a mirror for R packages\n",
    "utils.chooseCRANmirror(ind=1) # select the first mirror in the list\n",
    "\n",
    "# R package names\n",
    "packnames = ('TDA')\n",
    "\n",
    "# R vector of strings\n",
    "from rpy2.robjects.vectors import StrVector\n",
    "\n",
    "utils.install_packages(StrVector('TDA'))\n",
    "r('library(TDA)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2\n",
    "from rpy2.robjects import pandas2ri # install any dependency package if you get error like \"module not found\"\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "\n",
    "def r_convert(x):\n",
    "    pandas2ri.activate()\n",
    "\n",
    "    with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "        r_df = ro.conversion.py2rpy(x)\n",
    "\n",
    "    from rpy2.robjects import globalenv\n",
    "    globalenv['r_df'] = r_df\n",
    "    r('Diag <- ripsDiag(X = r_df, 1, max(r_df), library = \"GUDHI\", printProgress = FALSE)')\n",
    "    return(r('Diag$diagram'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "comedy_rips = comedy_test.groupby(comedy_test.index).apply(lambda x: r_convert(x))\n",
    "horror_rips = horror_test.groupby(horror_test.index).apply(lambda x: r_convert(x))\n",
    "documentary_rips = documentary_test.groupby(documentary_test.index).apply(lambda x: r_convert(x))\n",
    "drama_rips = drama_test.groupby(drama_test.index).apply(lambda x: r_convert(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23     [[0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, ...\n",
       "24     [[0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, ...\n",
       "25                    [[0.0, 0.0, 1.0], [0.0, 0.0, 1.0]]\n",
       "26     [[0.0, 0.0, 2.0], [0.0, 0.0, 2.0], [0.0, 0.0, ...\n",
       "27     [[0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, ...\n",
       "                             ...                        \n",
       "409                                    [[0.0, 0.0, 0.0]]\n",
       "410                   [[0.0, 0.0, 1.0], [0.0, 0.0, 1.0]]\n",
       "411                   [[0.0, 0.0, 1.0], [0.0, 0.0, 1.0]]\n",
       "412                   [[0.0, 0.0, 1.0], [0.0, 0.0, 1.0]]\n",
       "413                                    [[0.0, 0.0, 0.0]]\n",
       "Length: 383, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comedy_rips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comedy_rips[comedy_rips.apply(lambda x: np.any(x[:, 0] == 1)).values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(comedy_rips.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: dev.new(): using pdf(file=\"Rplots1.pdf\")\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int32)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r('dev.new()') # optional: create a new figure\n",
    "r('plot(Diag$diagram, barcode=TRUE)')\n",
    "r('dev.off()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.57079633, 1.57079633, 1.57079633, 1.57079633],\n",
       "       [1.57079633, 0.        , 0.95531695, 1.57079633, 1.57079633],\n",
       "       [1.57079633, 0.95531695, 0.        , 1.57079633, 1.23096295],\n",
       "       [1.57079633, 1.57079633, 1.57079633, 0.        , 1.57079633],\n",
       "       [1.57079633, 1.57079633, 1.23096295, 1.57079633, 0.        ]])"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arccos(np.round(cosine_similarity(comedy_test.loc[46]), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in arccos\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.10734243e-08, 1.57079633e+00, 1.57079633e+00, 1.57079633e+00,\n",
       "        1.57079633e+00],\n",
       "       [1.57079633e+00, 0.00000000e+00, 9.55316618e-01, 1.57079633e+00,\n",
       "        1.57079633e+00],\n",
       "       [1.57079633e+00, 9.55316618e-01,            nan, 1.57079633e+00,\n",
       "        1.23095942e+00],\n",
       "       [1.57079633e+00, 1.57079633e+00, 1.57079633e+00, 0.00000000e+00,\n",
       "        1.57079633e+00],\n",
       "       [1.57079633e+00, 1.57079633e+00, 1.23095942e+00, 1.57079633e+00,\n",
       "                   nan]])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "np.arccos(cosine_similarity(comedy_test.loc[46]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comedy_test.loc[46].sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documentary_test.loc[46].sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drama_test.loc[46].sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "horror_test.loc[46].sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r('dev.off()')\n",
    "r('dev.new()') # optional: create a new figure\n",
    "r('plot(diag.info$diagram, barcode=TRUE)')\n",
    "r('dev.off()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df = df.iloc[:, (groups_filter.apply(lambda x: sum(x != 0)) == 1).values]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "tfidf = transformer.fit_transform(df)\n",
    "tfidf = pd.DataFrame(tfidf.toarray())\n",
    "tfidf.columns = df.columns.difference(['Genre 1'])\n",
    "tfidf\n",
    "\n",
    "tfidf = tfidf.reset_index()[tfidf.reset_index().columns.difference(['index'])]\n",
    "first_class = pd.concat([dtm, tfidf], axis=1)\n",
    "first_class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
