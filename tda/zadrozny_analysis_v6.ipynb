{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import & Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "from langdetect import detect\n",
    "\n",
    "\n",
    "path = '../tda'\n",
    "os.chdir(path)\n",
    "\n",
    "movies = pd.read_csv('movies_genres.csv', delimiter='\\t')\n",
    "movies['plot_lang'] = movies.apply(lambda row: detect(row['plot']), axis=1)\n",
    "movies = movies[movies.plot_lang.isin(['en'])]\n",
    "\n",
    "# Only 1 genre\n",
    "movies = movies[movies.iloc[:, 2:30].sum(axis = 1) == 1]\n",
    "genres = movies.iloc[:, 2:30].replace(1, pd.Series(movies.columns, movies.columns)).replace(0, \"\").max(axis = 1)\n",
    "movies = pd.concat([movies[['title', 'plot']], genres], axis = 1).rename(columns={0:'genre'})\n",
    "\n",
    "# Get rid of strange dots, replace question & exclamation marks\n",
    "movies.loc['plot'] = movies['plot'].str.replace('\\\\.\\\\.\\\\.', '\\\\.')\n",
    "movies.loc['plot'] = movies['plot'].str.replace('\\\\.\\\\.', '\\\\.')\n",
    "movies.loc['plot'] = movies['plot'].str.replace('Mrs\\\\.', 'Mrs')\n",
    "movies.loc['plot'] = movies['plot'].str.replace('Mr\\\\.', 'Mr')\n",
    "movies.loc['plot'] = movies['plot'].str.replace('\\\\?', '\\\\.')\n",
    "movies.loc['plot'] = movies['plot'].str.replace('\\\\!', '\\\\.')\n",
    "\n",
    "# More than 3 sentences\n",
    "movies = movies.iloc[((~movies['plot'].str.split('\\\\. ',expand=True).isnull()).apply(sum, axis = 1) > 3).values]\n",
    "\n",
    "# Genre is Comedy, Documentary or Drama\n",
    "movies = movies[movies['genre'].apply(lambda x: x in ['Comedy', 'Documentary', 'Drama'])]\n",
    "\n",
    "# Choosing base set\n",
    "movies = pd.concat([movies[movies['genre'] == 'Comedy'].sample(2800, random_state = 1),\n",
    "movies[movies['genre'] == 'Documentary'].sample(2800, random_state = 2),\n",
    "movies[movies['genre'] == 'Drama'].sample(2800, random_state = 3)])\n",
    "\n",
    "# Choosing test set\n",
    "movies_test = pd.concat([movies[movies['genre'] == 'Comedy'].sample(200, random_state = 4),\n",
    "movies[movies['genre'] == 'Documentary'].sample(200, random_state = 5),\n",
    "movies[movies['genre'] == 'Drama'].sample(200, random_state = 6)])\n",
    "\n",
    "# Splitting test by sentence\n",
    "sentences = pd.DataFrame(movies_test['plot'].str.split('\\\\. ',expand=True).unstack()).reset_index().sort_values(['level_1', 'level_0'])\n",
    "sentences = sentences[sentences[0].apply(lambda x: x is not None)]\n",
    "sentences = sentences.set_index('level_1').drop('level_0', axis = 1).rename(columns = {0:'plot'})\n",
    "\n",
    "movies = movies.loc[np.setdiff1d(movies.index.values, movies_test.index.values)]\n",
    "\n",
    "# Genres\n",
    "genres = movies['genre']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            i\n",
       "1           me\n",
       "2           my\n",
       "3       myself\n",
       "4           we\n",
       "        ...   \n",
       "174     werent\n",
       "175        won\n",
       "176       wont\n",
       "177     wouldn\n",
       "178    wouldnt\n",
       "Length: 179, dtype: object"
      ]
     },
     "execution_count": 904,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = pd.Series(stop).apply(lambda x: x.translate(str.maketrans({key: None for key in string.punctuation})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'apply'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-905-bc03caa59c16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'apply'"
     ]
    }
   ],
   "source": [
    "stop = stop.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'translate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-901-3b39b37227a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# docs = docs.apply(lambda x: x.lower())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'translate'"
     ]
    }
   ],
   "source": [
    "stop.translate(str.maketrans({key: None for key in string.punctuation}))\n",
    "# docs = docs.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "def mj_dtm(description):\n",
    "    \n",
    "    def get_wordnet_pos(word):\n",
    "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "    \n",
    "    stop = stopwords.words('english')\n",
    "\n",
    "    docs = description\n",
    "\n",
    "    docs = docs.apply(lambda x: x.translate(str.maketrans({key: None for key in string.punctuation})))\n",
    "    docs = docs.apply(lambda x: x.lower())\n",
    "\n",
    "    docs = docs.apply(lambda x: x.split())\n",
    "    \n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    docs = docs.apply(lambda x: [wordnet_lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in x])\n",
    "    \n",
    "#     lancaster = LancasterStemmer()\n",
    "#     docs = docs.apply(lambda x: [lancaster.stem(word) for word in x])\n",
    "\n",
    "    docs = docs.apply(lambda x: [word for word in x if word not in stop])\n",
    "    \n",
    "    docs = docs.apply(lambda x: ' '.join(x))\n",
    "\n",
    "    vec = CountVectorizer()\n",
    "\n",
    "    X = vec.fit_transform(docs)\n",
    "    df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names(), index = docs.index)\n",
    "\n",
    "    # Words occurred in more than 1 movie\n",
    "#     df = df.iloc[:,((df>0).apply(sum) > 1).values]\n",
    "\n",
    "    df = df.iloc[:, pd.Series(df.columns).apply(lambda x: re.match(\"^[0-9]\", x) is None).values]\n",
    "    df = df.iloc[:, pd.Series(df.columns).apply(lambda x: re.match(\"^[a-z]\", x) is not None).values]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mj_dtm(movies['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_movies = pd.concat([genres, df.sum(axis = 1)], axis = 1).groupby('Genre 1')\\\n",
    ".apply(lambda x: x.sort_values(ascending = False, by = 0)[:1600]).index.get_level_values(None).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = movies.loc[top_movies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mj_dtm(movies['Description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing genre top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "groups_filter = pd.concat([movies['Genre 1'], df], axis=1)\n",
    "sum_groups = groups_filter.groupby('Genre 1').sum()\n",
    "sum_overall = sum_groups.sum()\n",
    "# categorical_cross_entropy = (1 - (sum_groups * np.log(sum_groups/sum_overall))/(sum_overall*np.log(1/3))).fillna(0)\n",
    "# categorical_cross_entropy\n",
    "categorical_shares = sum_groups/sum_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Genre 1\n",
       "Comedy         1600.0\n",
       "Documentary    1600.0\n",
       "Drama          1600.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(groups_filter.set_index('Genre 1').sum(axis = 1) > 40).groupby('Genre 1').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "comedy_top_words = categorical_shares.loc['Comedy'][sum_groups.loc['Comedy'].sort_values(ascending = False)[:500]\\\n",
    "                                                    .index.values].sort_values(ascending = False)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stooge      1.000000\n",
       "det         0.983607\n",
       "que         0.968254\n",
       "standup     0.943182\n",
       "til         0.936170\n",
       "              ...   \n",
       "teach       0.378947\n",
       "learns      0.377049\n",
       "star        0.376884\n",
       "another     0.376068\n",
       "everyone    0.375000\n",
       "Name: Comedy, Length: 200, dtype: float64"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comedy_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentary_top_words = categorical_shares.loc['Documentary'][sum_groups.loc['Documentary'].sort_values(ascending = False)[:500]\\\n",
    "                                                              .index.values].sort_values(ascending = False)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "documentary    0.944538\n",
       "examines       0.923077\n",
       "wrestling      0.893939\n",
       "document       0.875000\n",
       "insight        0.868421\n",
       "                 ...   \n",
       "issue          0.504274\n",
       "war            0.503371\n",
       "understand     0.500000\n",
       "building       0.500000\n",
       "york           0.497738\n",
       "Name: Documentary, Length: 200, dtype: float64"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documentary_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "drama_top_words = categorical_shares.loc['Drama'][sum_groups.loc['Drama'].sort_values(ascending = False)[:500]\\\n",
    "                                                  .index.values].sort_values(ascending = False)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "anna       0.752941\n",
       "drama      0.681564\n",
       "tragedy    0.666667\n",
       "unable     0.666667\n",
       "dy         0.663265\n",
       "             ...   \n",
       "later      0.412121\n",
       "drug       0.410811\n",
       "act        0.410811\n",
       "door       0.410000\n",
       "name       0.409922\n",
       "Name: Drama, Length: 200, dtype: float64"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drama_top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boyfriend',\n",
       " 'manage',\n",
       " 'seem',\n",
       " 'let',\n",
       " 'win',\n",
       " 'trip',\n",
       " 'want',\n",
       " 'however',\n",
       " 'shes']"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection(mj_dtm(sentences[:5]['overview']).columns.values, comedy_top_words.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersecter(x, base):\n",
    "    res = mj_dtm(x)\n",
    "    res = res[intersection(res.columns.values, base.index.values)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def calculate_dist_matrix(x):\n",
    "    \n",
    "    if len(x.columns) == 0:\n",
    "        return np.array([[0]])\n",
    "    else:\n",
    "        dist_matrix = np.arccos(np.round(cosine_similarity(x), 5))\n",
    "        aux_zeros = np.zeros(dist_matrix.shape[0] - 1)\n",
    "        np.fill_diagonal(dist_matrix[1:], aux_zeros)\n",
    "        np.fill_diagonal(dist_matrix[:,1:], aux_zeros)\n",
    "        np.fill_diagonal(dist_matrix, 0)\n",
    "        return dist_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/rpy2/robjects/pandas2ri.py:197: UserWarning: R object inheriting from \"POSIXct\" but without attribute \"tzone\".\n",
      "  warnings.warn('R object inheriting from \"POSIXct\" but without '\n",
      "R[write to console]: Warning:\n",
      "R[write to console]:  failed to download mirrors file (cannot download all files); using local file '/Library/Frameworks/R.framework/Resources/doc/CRAN_mirrors.csv'\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['TDA', 'tools', 'stats', 'graphics', 'grDevices', 'utils',\n",
       "       'datasets', 'methods', 'base'], dtype='<U9')"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy as sp\n",
    "from rpy2.robjects.packages import importr\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import r\n",
    "# import pandas.rpy.common as com\n",
    "\n",
    "# import R's \"base\" package\n",
    "base = importr('base')\n",
    "\n",
    "# import R's \"utils\" package\n",
    "utils = importr('utils')\n",
    "\n",
    "# import rpy2's package module\n",
    "import rpy2.robjects.packages as rpackages\n",
    "\n",
    "# import R's utility package\n",
    "utils = rpackages.importr('utils')\n",
    "\n",
    "# select a mirror for R packages\n",
    "utils.chooseCRANmirror(ind=1) # select the first mirror in the list\n",
    "\n",
    "# R package names\n",
    "packnames = ('TDA')\n",
    "\n",
    "# R vector of strings\n",
    "from rpy2.robjects.vectors import StrVector\n",
    "\n",
    "utils.install_packages(StrVector('TDA'))\n",
    "r('library(TDA)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2\n",
    "from rpy2.robjects import pandas2ri # install any dependency package if you get error like \"module not found\"\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "from rpy2.robjects import globalenv\n",
    "\n",
    "def r_convert(x):\n",
    "    pandas2ri.activate()\n",
    "    \n",
    "    aux_tmp = pd.Series(x.tolist()).loc[0][0]\n",
    "    \n",
    "    with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "        r_df = ro.conversion.py2rpy(aux_tmp)\n",
    "        \n",
    "    globalenv['r_df'] = r_df\n",
    "    r('Diag <- ripsDiag(X = r_df, 1, max(r_df), library = \"Dionysus\", dist = \"arbitrary\", printProgress = FALSE)')\n",
    "    return(r('Diag$diagram'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.]])"
      ]
     },
     "execution_count": 752,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_convert(calculate_dist_matrix(intersecter(sentences.loc[2024]['overview'], comedy_top_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level_1\n",
       "2024     [[0.0, 0.0, 0.0]]\n",
       "2030     [[0.0, 0.0, 0.0]]\n",
       "2057     [[0.0, 0.0, 0.0]]\n",
       "2062     [[0.0, 0.0, 0.0]]\n",
       "2096     [[0.0, 0.0, 0.0]]\n",
       "               ...        \n",
       "17871    [[0.0, 0.0, 0.0]]\n",
       "17891    [[0.0, 0.0, 0.0]]\n",
       "17892    [[0.0, 0.0, 0.0]]\n",
       "18041    [[0.0, 0.0, 0.0]]\n",
       "18064    [[0.0, 0.0, 0.0]]\n",
       "Length: 600, dtype: object"
      ]
     },
     "execution_count": 733,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comedy_rips = sentences.groupby('level_1')\\\n",
    ".apply(lambda x: r_convert(calculate_dist_matrix(intersecter(x['overview'], comedy_top_words))))\n",
    "comedy_rips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level_1\n",
       "2024     [[0.0, 0.0, 0.0]]\n",
       "2030     [[0.0, 0.0, 0.0]]\n",
       "2057     [[0.0, 0.0, 0.0]]\n",
       "2062     [[0.0, 0.0, 0.0]]\n",
       "2096     [[0.0, 0.0, 0.0]]\n",
       "               ...        \n",
       "17871    [[0.0, 0.0, 0.0]]\n",
       "17891    [[0.0, 0.0, 0.0]]\n",
       "17892    [[0.0, 0.0, 0.0]]\n",
       "18041    [[0.0, 0.0, 0.0]]\n",
       "18064    [[0.0, 0.0, 0.0]]\n",
       "Length: 600, dtype: object"
      ]
     },
     "execution_count": 734,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documentary_rips = sentences.groupby('level_1')\\\n",
    ".apply(lambda x: r_convert(calculate_dist_matrix(intersecter(x['overview'], documentary_top_words))))\n",
    "documentary_rips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level_1\n",
       "2024     [[0.0, 0.0, 0.0]]\n",
       "2030     [[0.0, 0.0, 0.0]]\n",
       "2057     [[0.0, 0.0, 0.0]]\n",
       "2062     [[0.0, 0.0, 0.0]]\n",
       "2096     [[0.0, 0.0, 0.0]]\n",
       "               ...        \n",
       "17871    [[0.0, 0.0, 0.0]]\n",
       "17891    [[0.0, 0.0, 0.0]]\n",
       "17892    [[0.0, 0.0, 0.0]]\n",
       "18041    [[0.0, 0.0, 0.0]]\n",
       "18064    [[0.0, 0.0, 0.0]]\n",
       "Length: 600, dtype: object"
      ]
     },
     "execution_count": 735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drama_rips = sentences.groupby('level_1')\\\n",
    ".apply(lambda x: r_convert(calculate_dist_matrix(intersecter(x['overview'], drama_top_words))))\n",
    "drama_rips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "print(comedy_rips.apply(lambda x: np.any(x[:, 0] == 1)).values.mean(),\n",
    "documentary_rips.apply(lambda x: np.any(x[:, 0] == 1)).values.mean(),\n",
    "drama_rips.apply(lambda x: np.any(x[:, 0] == 1)).values.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3411444141689373"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(movies.loc[comedy_rips[comedy_rips.apply(lambda x: np.any(x[:, 0] == 1)).values].index.values]['Genre 1'] == 'Comedy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6551528878822197"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(movies.loc[documentary_rips[documentary_rips.apply(lambda x: np.any(x[:, 0] == 1)).values].index.values]['Genre 1'] == 'Documentary').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48909487459105777"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(movies.loc[drama_rips[drama_rips.apply(lambda x: np.any(x[:, 0] == 1)).values].index.values]['Genre 1'] == 'Drama').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: dev.new(): using pdf(file=\"Rplots1.pdf\")\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int32)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r('dev.new()') # optional: create a new figure\n",
    "r('plot(Diag$diagram, barcode=TRUE)')\n",
    "r('dev.off()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
