{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import & Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "path = '../tda'\n",
    "os.chdir(path)\n",
    "\n",
    "movies = pd.read_csv('AllMoviesDetailsCleaned.csv', encoding = 'utf8', sep = \";\")\n",
    "movies.dropna(subset=['overview'], inplace=True)\n",
    "movies.apply(lambda x: pd.api.types.infer_dtype(x.values))\n",
    "\n",
    "\n",
    "genres = movies['genres'].str.split('|',expand=True)\n",
    "genres = genres.values\n",
    "genres = pd.DataFrame(genres)\n",
    "genres.columns = ['Genre 1', 'Genre 2', 'Genre 3', 'Genre 4', 'Genre 5', 'Genre 6', 'Genre 7', 'Genre 8', 'Genre 9', 'Genre 10', 'Genre 11']\n",
    "genres = genres.applymap(lambda x: '' if x is None else str(x))\n",
    "\n",
    "# Only 1 genre\n",
    "movies = movies.iloc[(genres.apply(lambda x: sum(x != ''), axis = 1) == 1).values]\n",
    "movies = movies.rename(columns = {'genres':'Genre 1', 'overview':'Description'})\n",
    "\n",
    "# Get rid of Mr. and Mrs. split\n",
    "movies['Description'] = movies['Description'].str.replace('Mrs\\\\.', 'Mrs')\n",
    "movies['Description'] = movies['Description'].str.replace('Mr\\\\.', 'Mr')\n",
    "\n",
    "# More than 3 sentences\n",
    "movies = movies.iloc[((~movies['Description'].str.split('\\\\. ',expand=True).isnull()).apply(sum, axis = 1) > 3).values]\n",
    "\n",
    "# Both genre and description not null\n",
    "movies = movies.iloc[((movies[['Genre 1', 'Description']].isnull()).apply(sum, axis = 1) == 0).values]\n",
    "\n",
    "# Genre share at least 10%\n",
    "movies = movies.set_index('Genre 1').join(pd.DataFrame(movies.groupby('Genre 1').size()/len(movies) > 0.1)).rename(columns = {0:'to_drop'}).reset_index()\n",
    "\n",
    "movies = movies.iloc[movies['to_drop'].values]\n",
    "movies = movies.drop(['to_drop'], axis = 1)\n",
    "\n",
    "# Choosing test set\n",
    "movies_test = pd.concat([movies[movies['Genre 1'] == 'Comedy'].sample(200, random_state = 1),\n",
    "movies[movies['Genre 1'] == 'Documentary'].sample(200, random_state = 2),\n",
    "movies[movies['Genre 1'] == 'Drama'].sample(200, random_state = 3)])\n",
    "\n",
    "# Splitting test by sentence\n",
    "sentences = pd.DataFrame(movies_test['Description'].str.split('\\\\. ',expand=True).unstack()).reset_index().sort_values(['level_1', 'level_0'])\n",
    "sentences = sentences[sentences[0].apply(lambda x: x is not None)]\n",
    "sentences = sentences.set_index('level_1').drop('level_0', axis = 1).rename(columns = {0:'overview'})\n",
    "\n",
    "movies = movies.loc[np.setdiff1d(movies.index.values, movies_test.index.values)]\n",
    "\n",
    "# Genres\n",
    "genres = movies['Genre 1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "def mj_dtm(description):\n",
    "    \n",
    "    def get_wordnet_pos(word):\n",
    "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "    \n",
    "    stop = stopwords.words('english')\n",
    "\n",
    "    docs = description\n",
    "\n",
    "    docs = docs.apply(lambda x: x.translate(str.maketrans({key: None for key in string.punctuation})))\n",
    "    docs = docs.apply(lambda x: x.lower())\n",
    "\n",
    "    docs = docs.apply(lambda x: x.split())\n",
    "    \n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    docs = docs.apply(lambda x: [wordnet_lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in x])\n",
    "    \n",
    "#     lancaster = LancasterStemmer()\n",
    "#     docs = docs.apply(lambda x: [lancaster.stem(word) for word in x])\n",
    "\n",
    "    docs = docs.apply(lambda x: [word for word in x if word not in stop])\n",
    "    \n",
    "    docs = docs.apply(lambda x: ' '.join(x))\n",
    "\n",
    "    vec = CountVectorizer()\n",
    "\n",
    "    X = vec.fit_transform(docs)\n",
    "    df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names(), index = docs.index)\n",
    "\n",
    "    # Words occurred in more than 1 movie\n",
    "#     df = df.iloc[:,((df>0).apply(sum) > 1).values]\n",
    "\n",
    "    df = df.iloc[:, pd.Series(df.columns).apply(lambda x: re.match(\"^[0-9]\", x) is None).values]\n",
    "    df = df.iloc[:, pd.Series(df.columns).apply(lambda x: re.match(\"^[a-z]\", x) is not None).values]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mj_dtm(movies['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_movies = pd.concat([genres, df.sum(axis = 1)], axis = 1).groupby('Genre 1')\\\n",
    ".apply(lambda x: x.sort_values(ascending = False, by = 0)[:1600]).index.get_level_values(None).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = movies.loc[top_movies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mj_dtm(movies['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_test = mj_dtm(sentences['overview'])\n",
    "# df_test = df_test.set_index(sentences.index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "groups_filter = pd.concat([movies['Genre 1'], df], axis=1)\n",
    "sum_groups = groups_filter.groupby('Genre 1').sum()\n",
    "sum_overall = sum_groups.sum()\n",
    "# categorical_cross_entropy = (1 - (sum_groups * np.log(sum_groups/sum_overall))/(sum_overall*np.log(1/3))).fillna(0)\n",
    "# categorical_cross_entropy\n",
    "categorical_shares = sum_groups/sum_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Genre 1\n",
       "Comedy         1600.0\n",
       "Documentary    1600.0\n",
       "Drama          1600.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(groups_filter.set_index('Genre 1').sum(axis = 1) > 40).groupby('Genre 1').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing genre top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "comedy_top_words = categorical_shares.loc['Comedy'][sum_groups.loc['Comedy'].sort_values(ascending = False)[:500]\\\n",
    "                                                    .index.values].sort_values(ascending = False)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stooge      1.000000\n",
       "det         0.983607\n",
       "que         0.968254\n",
       "standup     0.943182\n",
       "til         0.936170\n",
       "              ...   \n",
       "teach       0.378947\n",
       "learns      0.377049\n",
       "star        0.376884\n",
       "another     0.376068\n",
       "everyone    0.375000\n",
       "Name: Comedy, Length: 200, dtype: float64"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comedy_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentary_top_words = categorical_shares.loc['Documentary'][sum_groups.loc['Documentary'].sort_values(ascending = False)[:500]\\\n",
    "                                                              .index.values].sort_values(ascending = False)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "documentary    0.944538\n",
       "examines       0.923077\n",
       "wrestling      0.893939\n",
       "document       0.875000\n",
       "insight        0.868421\n",
       "                 ...   \n",
       "issue          0.504274\n",
       "war            0.503371\n",
       "understand     0.500000\n",
       "building       0.500000\n",
       "york           0.497738\n",
       "Name: Documentary, Length: 200, dtype: float64"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documentary_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "drama_top_words = categorical_shares.loc['Drama'][sum_groups.loc['Drama'].sort_values(ascending = False)[:500]\\\n",
    "                                                  .index.values].sort_values(ascending = False)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "anna       0.752941\n",
       "drama      0.681564\n",
       "tragedy    0.666667\n",
       "unable     0.666667\n",
       "dy         0.663265\n",
       "             ...   \n",
       "later      0.412121\n",
       "drug       0.410811\n",
       "act        0.410811\n",
       "door       0.410000\n",
       "name       0.409922\n",
       "Name: Drama, Length: 200, dtype: float64"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drama_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def intersection(lst1, lst2): \n",
    "#     return list(set(lst1) & set(lst2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def summation(lst1, lst2): \n",
    "#     return list(set(lst1) | set(lst2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_top_words = summation(\n",
    "#     summation(\n",
    "#     intersection(\n",
    "#         comedy_top_words.index.values, \n",
    "#         documentary_top_words.index.values), \n",
    "#     intersection(\n",
    "#         comedy_top_words.index.values, \n",
    "#         drama_top_words.index.values)),\n",
    "#     intersection(documentary_top_words.index.values,\n",
    "#                 drama_top_words.index.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(np.setdiff1d(comedy_top_words.index.values, common_top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(np.setdiff1d(documentary_top_words.index.values, common_top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(np.setdiff1d(drama_top_words.index.values, common_top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# comedy_top_words = comedy_top_words[np.setdiff1d(comedy_top_words.index.values, common_top_words)].sort_values(ascending = False)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documentary_top_words = documentary_top_words[np.setdiff1d(documentary_top_words.index.values, common_top_words)].sort_values(ascending = False)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drama_top_words = drama_top_words[np.setdiff1d(drama_top_words.index.values, common_top_words)].sort_values(ascending = False)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stooge      1.000000\n",
       "så          1.000000\n",
       "blondie     1.000000\n",
       "det         0.985201\n",
       "curly       0.972004\n",
       "              ...   \n",
       "board       0.665483\n",
       "sit         0.665433\n",
       "thats       0.665204\n",
       "queen       0.665184\n",
       "solution    0.665142\n",
       "Name: Comedy, Length: 193, dtype: float64"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comedy_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dinosaur        1.000000\n",
       "specie          1.000000\n",
       "archival        0.976962\n",
       "historian       0.969160\n",
       "documentary     0.950943\n",
       "                  ...   \n",
       "rap             0.711500\n",
       "conversation    0.710149\n",
       "transform       0.709989\n",
       "site            0.708923\n",
       "gold            0.708923\n",
       "Name: Documentary, Length: 200, dtype: float64"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# documentary_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vijay       0.921132\n",
       "troubled    0.833897\n",
       "grandson    0.831656\n",
       "jealousy    0.819643\n",
       "fiancé      0.809240\n",
       "              ...   \n",
       "theme       0.665286\n",
       "spread      0.665252\n",
       "courage     0.665252\n",
       "cost        0.665184\n",
       "johnny      0.665173\n",
       "Name: Drama, Length: 200, dtype: float64"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drama_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beirut',\n",
       " 'bench',\n",
       " 'berry',\n",
       " 'biology',\n",
       " 'blackmails',\n",
       " 'bombings',\n",
       " 'booming',\n",
       " 'bout',\n",
       " 'bow',\n",
       " 'branded',\n",
       " 'brothels',\n",
       " 'brutality',\n",
       " 'bryan',\n",
       " 'bumbling',\n",
       " 'burlesque',\n",
       " 'butterfly',\n",
       " 'ca',\n",
       " 'cambodia',\n",
       " 'cameraman',\n",
       " 'censorship',\n",
       " 'champ',\n",
       " 'chelsea',\n",
       " 'chernobyl',\n",
       " 'churches',\n",
       " 'cindy',\n",
       " 'clinton',\n",
       " 'coincidence',\n",
       " 'colin',\n",
       " 'collage',\n",
       " 'collected',\n",
       " 'competitions',\n",
       " 'compromising',\n",
       " 'congolese',\n",
       " 'consequently',\n",
       " 'consumption',\n",
       " 'contend',\n",
       " 'counts',\n",
       " 'crashed',\n",
       " 'crashing',\n",
       " 'creators',\n",
       " 'crowds',\n",
       " 'dakota',\n",
       " 'dana',\n",
       " 'dash',\n",
       " 'declares',\n",
       " 'deer',\n",
       " 'dense',\n",
       " 'designers',\n",
       " 'devi',\n",
       " 'devices',\n",
       " 'diego',\n",
       " 'differently',\n",
       " 'dim',\n",
       " 'dimensions',\n",
       " 'dismay',\n",
       " 'documentation',\n",
       " 'donkey',\n",
       " 'donna',\n",
       " 'dumped',\n",
       " 'eclectic',\n",
       " 'educate',\n",
       " 'ein',\n",
       " 'eleanor',\n",
       " 'eli',\n",
       " 'embarrassment',\n",
       " 'emptiness',\n",
       " 'ethics',\n",
       " 'eugene',\n",
       " 'examined',\n",
       " 'exception',\n",
       " 'expense',\n",
       " 'fait',\n",
       " 'falsely',\n",
       " 'fare',\n",
       " 'fascist',\n",
       " 'fault',\n",
       " 'fearing',\n",
       " 'feisty',\n",
       " 'file',\n",
       " 'fingers',\n",
       " 'flights',\n",
       " 'floating',\n",
       " 'foil',\n",
       " 'foremost',\n",
       " 'formal',\n",
       " 'founders',\n",
       " 'fragility',\n",
       " 'fraught',\n",
       " 'freight',\n",
       " 'frost',\n",
       " 'function',\n",
       " 'fury',\n",
       " 'får',\n",
       " 'generous',\n",
       " 'gesture',\n",
       " 'globalization',\n",
       " 'grandma',\n",
       " 'guerrilla',\n",
       " 'ha',\n",
       " 'hairdresser',\n",
       " 'handles',\n",
       " 'hapless',\n",
       " 'harassed',\n",
       " 'hector',\n",
       " 'heels',\n",
       " 'hermann',\n",
       " 'himalayas',\n",
       " 'hiv',\n",
       " 'hoffman',\n",
       " 'hooked',\n",
       " 'hurry',\n",
       " 'ideological',\n",
       " 'ideology',\n",
       " 'igor',\n",
       " 'imagines',\n",
       " 'indifference',\n",
       " 'inhabit',\n",
       " 'injuries',\n",
       " 'intersect',\n",
       " 'intimately',\n",
       " 'invest',\n",
       " 'investigators',\n",
       " 'involvement',\n",
       " 'irreverent',\n",
       " 'jayaram',\n",
       " 'jeffrey',\n",
       " 'jess',\n",
       " 'julio',\n",
       " 'jungles',\n",
       " 'katie',\n",
       " 'katrina',\n",
       " 'kick',\n",
       " 'km',\n",
       " 'kun',\n",
       " 'lacking',\n",
       " 'launching',\n",
       " 'lauren',\n",
       " 'layer',\n",
       " 'lees',\n",
       " 'lisbon',\n",
       " 'lock',\n",
       " 'loner',\n",
       " 'loser',\n",
       " 'losers',\n",
       " 'mall',\n",
       " 'managing',\n",
       " 'mandela',\n",
       " 'marilyn',\n",
       " 'marsh',\n",
       " 'mathilde',\n",
       " 'megan',\n",
       " 'miki',\n",
       " 'militant',\n",
       " 'mundane',\n",
       " 'murderers',\n",
       " 'neal',\n",
       " 'networks',\n",
       " 'nobleman',\n",
       " 'oppressive',\n",
       " 'oral',\n",
       " 'organizes',\n",
       " 'outsiders',\n",
       " 'outspoken',\n",
       " 'overweight',\n",
       " 'ownership',\n",
       " 'pakistan',\n",
       " 'partying',\n",
       " 'passport',\n",
       " 'patrons',\n",
       " 'percy',\n",
       " 'permanently',\n",
       " 'physics',\n",
       " 'pile',\n",
       " 'possessed',\n",
       " 'possessions',\n",
       " 'postman',\n",
       " 'powered',\n",
       " 'precarious',\n",
       " 'promptly',\n",
       " 'prospective',\n",
       " 'protective',\n",
       " 'punished',\n",
       " 'quil',\n",
       " 'rama',\n",
       " 'reagan',\n",
       " 'reappears',\n",
       " 'reference',\n",
       " 'refined',\n",
       " 'remarkably',\n",
       " 'reporting',\n",
       " 'restoration',\n",
       " 'reunites',\n",
       " 'rev',\n",
       " 'ridiculous',\n",
       " 'rites',\n",
       " 'roam',\n",
       " 'robbing',\n",
       " 'rwanda',\n",
       " 'sacrifices',\n",
       " 'sakura',\n",
       " 'santos',\n",
       " 'scam',\n",
       " 'scholar',\n",
       " 'scottish',\n",
       " 'scrap',\n",
       " 'sections',\n",
       " 'secular',\n",
       " 'seed',\n",
       " 'sensational',\n",
       " 'separately',\n",
       " 'sheds',\n",
       " 'shine',\n",
       " 'shoe',\n",
       " 'shoulders',\n",
       " 'sincere',\n",
       " 'sixth',\n",
       " 'skiers',\n",
       " 'somebody',\n",
       " 'soninlaw',\n",
       " 'sporting',\n",
       " 'statement',\n",
       " 'stevens',\n",
       " 'stirring',\n",
       " 'strain',\n",
       " 'stream',\n",
       " 'strive',\n",
       " 'sua',\n",
       " 'subway',\n",
       " 'suitcase',\n",
       " 'sunset',\n",
       " 'surround',\n",
       " 'syria',\n",
       " 'taxes',\n",
       " 'teammates',\n",
       " 'telugu',\n",
       " 'tide',\n",
       " 'to',\n",
       " 'tons',\n",
       " 'tout',\n",
       " 'townspeople',\n",
       " 'toxic',\n",
       " 'trailer',\n",
       " 'tramp',\n",
       " 'treacherous',\n",
       " 'trusted',\n",
       " 'urgent',\n",
       " 'vaudeville',\n",
       " 'venus',\n",
       " 'viewed',\n",
       " 'vignettes',\n",
       " 'wallet',\n",
       " 'wanda',\n",
       " 'weaving',\n",
       " 'welsh',\n",
       " 'whale',\n",
       " 'whore',\n",
       " 'worldly',\n",
       " 'yorks',\n",
       " 'abbey',\n",
       " 'abraham',\n",
       " 'abyss',\n",
       " 'accurate',\n",
       " 'accusations',\n",
       " 'acquainted',\n",
       " 'adjust',\n",
       " 'admiration',\n",
       " 'admirers',\n",
       " 'adolf',\n",
       " 'adorable',\n",
       " 'ads',\n",
       " 'adversity',\n",
       " 'ai',\n",
       " 'aided',\n",
       " 'algeria',\n",
       " 'alienation',\n",
       " 'alpine',\n",
       " 'alter',\n",
       " 'amusement',\n",
       " 'anand',\n",
       " 'anarchist',\n",
       " 'andré',\n",
       " 'announcement',\n",
       " 'appreciation',\n",
       " 'appropriate',\n",
       " 'approval',\n",
       " 'arrests',\n",
       " 'ashamed',\n",
       " 'auf',\n",
       " 'awry',\n",
       " 'bachelors',\n",
       " 'banner',\n",
       " 'beaches',\n",
       " 'belt',\n",
       " 'bets',\n",
       " 'biker',\n",
       " 'bin',\n",
       " 'blast',\n",
       " 'blessed',\n",
       " 'bombay',\n",
       " 'boots',\n",
       " 'booze',\n",
       " 'bosnia',\n",
       " 'broad',\n",
       " 'brotherhood',\n",
       " 'camille',\n",
       " 'capacity',\n",
       " 'casa',\n",
       " 'category',\n",
       " 'centered',\n",
       " 'charley',\n",
       " 'cheat',\n",
       " 'cheats',\n",
       " 'childbirth',\n",
       " 'chinas',\n",
       " 'chopper',\n",
       " 'cinematographic',\n",
       " 'civilian',\n",
       " 'cleaner',\n",
       " 'cocktail',\n",
       " 'coffin',\n",
       " 'com',\n",
       " 'communal',\n",
       " 'commune',\n",
       " 'computers',\n",
       " 'confess',\n",
       " 'contribution',\n",
       " 'convert',\n",
       " 'correct',\n",
       " 'couldn',\n",
       " 'coward',\n",
       " 'cowboys',\n",
       " 'cracks',\n",
       " 'cubs',\n",
       " 'culminates',\n",
       " 'daniels',\n",
       " 'darwin',\n",
       " 'deciding',\n",
       " 'decorated',\n",
       " 'defence',\n",
       " 'defending',\n",
       " 'delhi',\n",
       " 'deliberately',\n",
       " 'depend',\n",
       " 'depends',\n",
       " 'describing',\n",
       " 'desired',\n",
       " 'discipline',\n",
       " 'distress',\n",
       " 'doll',\n",
       " 'dolls',\n",
       " 'dos',\n",
       " 'dreamer',\n",
       " 'dune',\n",
       " 'eighties',\n",
       " 'embodies',\n",
       " 'emigrated',\n",
       " 'endurance',\n",
       " 'enterprise',\n",
       " 'equipped',\n",
       " 'estimated',\n",
       " 'evocative',\n",
       " 'evolves',\n",
       " 'existing',\n",
       " 'exposure',\n",
       " 'fabian',\n",
       " 'fairly',\n",
       " 'faraway',\n",
       " 'fascism',\n",
       " 'federation',\n",
       " 'festive',\n",
       " 'fits',\n",
       " 'flag',\n",
       " 'flew',\n",
       " 'flowing',\n",
       " 'forge',\n",
       " 'forties',\n",
       " 'foul',\n",
       " 'fritz',\n",
       " 'fueled',\n",
       " 'gamblers',\n",
       " 'genres',\n",
       " 'glasses',\n",
       " 'goa',\n",
       " 'graduates',\n",
       " 'grandchildren',\n",
       " 'habitat',\n",
       " 'halls',\n",
       " 'hana',\n",
       " 'handled',\n",
       " 'harvey',\n",
       " 'hatches',\n",
       " 'haunts',\n",
       " 'hectic',\n",
       " 'hid',\n",
       " 'hindi',\n",
       " 'hippie',\n",
       " 'houston',\n",
       " 'hut',\n",
       " 'idaho',\n",
       " 'idle',\n",
       " 'ignorance',\n",
       " 'illicit',\n",
       " 'ils',\n",
       " 'increases',\n",
       " 'inlaws',\n",
       " 'instance',\n",
       " 'intend',\n",
       " 'interaction',\n",
       " 'intertwine',\n",
       " 'intervention',\n",
       " 'ist',\n",
       " 'jade',\n",
       " 'jewels',\n",
       " 'judith',\n",
       " 'kai',\n",
       " 'kidnappers',\n",
       " 'killings',\n",
       " 'kyoto',\n",
       " 'landlady',\n",
       " 'lars',\n",
       " 'leung',\n",
       " 'liberated',\n",
       " 'lincoln',\n",
       " 'liquid',\n",
       " 'locate',\n",
       " 'logging',\n",
       " 'lorenzo',\n",
       " 'lorna',\n",
       " 'lovable',\n",
       " 'luca',\n",
       " 'lucrative',\n",
       " 'madonna',\n",
       " 'maintained',\n",
       " 'markus',\n",
       " 'marseille',\n",
       " 'masses',\n",
       " 'medal',\n",
       " 'meena',\n",
       " 'microcosm',\n",
       " 'mina',\n",
       " 'mirrors',\n",
       " 'misadventures',\n",
       " 'motivated',\n",
       " 'multinational',\n",
       " 'muse',\n",
       " 'nadia',\n",
       " 'nagasaki',\n",
       " 'nagging',\n",
       " 'nambiar',\n",
       " 'natalia',\n",
       " 'nathalie',\n",
       " 'nationwide',\n",
       " 'natures',\n",
       " 'nile',\n",
       " 'nineyearold',\n",
       " 'noticed',\n",
       " 'notion',\n",
       " 'objections',\n",
       " 'oblivion',\n",
       " 'observer',\n",
       " 'occult',\n",
       " 'optimism',\n",
       " 'optimistic',\n",
       " 'organised',\n",
       " 'organizing',\n",
       " 'osaka',\n",
       " 'overbearing',\n",
       " 'overdose',\n",
       " 'painfully',\n",
       " 'palmer',\n",
       " 'parker',\n",
       " 'peek',\n",
       " 'peru',\n",
       " 'pirate',\n",
       " 'pirates',\n",
       " 'plague',\n",
       " 'pleased',\n",
       " 'pledge',\n",
       " 'populated',\n",
       " 'premiered',\n",
       " 'presidency',\n",
       " 'probe',\n",
       " 'processes',\n",
       " 'proposed',\n",
       " 'prosecutor',\n",
       " 'queens',\n",
       " 'rail',\n",
       " 'rains',\n",
       " 'rays',\n",
       " 'realise',\n",
       " 'rebirth',\n",
       " 'regard',\n",
       " 'regiment',\n",
       " 'regulations',\n",
       " 'relate',\n",
       " 'renaissance',\n",
       " 'renoir',\n",
       " 'revive',\n",
       " 'robbers',\n",
       " 'robinson',\n",
       " 'rodney',\n",
       " 'rodriguez',\n",
       " 'rogue',\n",
       " 'rovi',\n",
       " 'ruben',\n",
       " 'rudolf',\n",
       " 'rundown',\n",
       " 'russians',\n",
       " 'sabine',\n",
       " 'sabotage',\n",
       " 'sadly',\n",
       " 'sailors',\n",
       " 'samantha',\n",
       " 'sammy',\n",
       " 'scars',\n",
       " 'scattered',\n",
       " 'screening',\n",
       " 'seducing',\n",
       " 'seeds',\n",
       " 'shawn',\n",
       " 'sinking',\n",
       " 'sisterinlaw',\n",
       " 'slacker',\n",
       " 'slips',\n",
       " 'slovak',\n",
       " 'smoothly',\n",
       " 'smuggled',\n",
       " 'snowboard',\n",
       " 'spanning',\n",
       " 'spin',\n",
       " 'spotted',\n",
       " 'spouse',\n",
       " 'stairs',\n",
       " 'statements',\n",
       " 'stephan',\n",
       " 'stepped',\n",
       " 'storage',\n",
       " 'superb',\n",
       " 'surely',\n",
       " 'sustainable',\n",
       " 'taboo',\n",
       " 'tanaka',\n",
       " 'tank',\n",
       " 'tap',\n",
       " 'tech',\n",
       " 'tempted',\n",
       " 'tenderness',\n",
       " 'tenyearold',\n",
       " 'terrified',\n",
       " 'terrorism',\n",
       " 'tremendous',\n",
       " 'tutor',\n",
       " 'undergoes',\n",
       " 'uneasy',\n",
       " 'unites',\n",
       " 'unsuspecting',\n",
       " 'untouched',\n",
       " 'urge',\n",
       " 'val',\n",
       " 'vampire',\n",
       " 'varma',\n",
       " 'vessel',\n",
       " 'vet',\n",
       " 'vita',\n",
       " 'volcanic',\n",
       " 'volcano',\n",
       " 'warmth',\n",
       " 'warns',\n",
       " 'wed',\n",
       " 'workplace',\n",
       " 'wound',\n",
       " 'xavier',\n",
       " 'yakuza',\n",
       " 'zu',\n",
       " 'är',\n",
       " 'år',\n",
       " 'по',\n",
       " 'accessible',\n",
       " 'admired',\n",
       " 'admits',\n",
       " 'admitted',\n",
       " 'adventurer',\n",
       " 'affecting',\n",
       " 'afghan',\n",
       " 'alexandre',\n",
       " 'alexis',\n",
       " 'altar',\n",
       " 'alzheimers',\n",
       " 'amazed',\n",
       " 'amira',\n",
       " 'ang',\n",
       " 'angle',\n",
       " 'angst',\n",
       " 'announce',\n",
       " 'appu',\n",
       " 'archaeologists',\n",
       " 'arguably',\n",
       " 'armand',\n",
       " 'associates',\n",
       " 'att',\n",
       " 'authenticity',\n",
       " 'authoritarian',\n",
       " 'autobiography',\n",
       " 'awake',\n",
       " 'awful',\n",
       " 'backs',\n",
       " 'banished',\n",
       " 'barren',\n",
       " 'bc',\n",
       " 'bennett',\n",
       " 'billionaire',\n",
       " 'bisexual',\n",
       " 'blessing',\n",
       " 'blown',\n",
       " 'boards',\n",
       " 'bosnian',\n",
       " 'boxes',\n",
       " 'breakfast',\n",
       " 'broadcasting',\n",
       " 'cairo',\n",
       " 'calf',\n",
       " 'camaraderie',\n",
       " 'capitalist',\n",
       " 'captains',\n",
       " 'captive',\n",
       " 'catalyst',\n",
       " 'cells',\n",
       " 'cent',\n",
       " 'centres',\n",
       " 'chamber',\n",
       " 'characterized',\n",
       " 'childs',\n",
       " 'cho',\n",
       " 'chorus',\n",
       " 'chronic',\n",
       " 'cigarettes',\n",
       " 'clandestine',\n",
       " 'climbed',\n",
       " 'closeup',\n",
       " 'collaborators',\n",
       " 'communists',\n",
       " 'competitor',\n",
       " 'complains',\n",
       " 'composition',\n",
       " 'confinement',\n",
       " 'constitution',\n",
       " 'contrasts',\n",
       " 'converge',\n",
       " 'convicts',\n",
       " 'counselor',\n",
       " 'coupled',\n",
       " 'coveted',\n",
       " 'crews',\n",
       " 'cries',\n",
       " 'crippled',\n",
       " 'criticism',\n",
       " 'culminating',\n",
       " 'cynthia',\n",
       " 'darker',\n",
       " 'deeds',\n",
       " 'defies',\n",
       " 'delicious',\n",
       " 'denies',\n",
       " 'developer',\n",
       " 'diagnosis',\n",
       " 'diamonds',\n",
       " 'dimension',\n",
       " 'displaying',\n",
       " 'distinct',\n",
       " 'ditch',\n",
       " 'dominique',\n",
       " 'draft',\n",
       " 'dressing',\n",
       " 'dropout',\n",
       " 'dual',\n",
       " 'dubai',\n",
       " 'dublin',\n",
       " 'duncan',\n",
       " 'ecstasy',\n",
       " 'edwards',\n",
       " 'elias',\n",
       " 'ellis',\n",
       " 'embarrassing',\n",
       " 'embittered',\n",
       " 'employers',\n",
       " 'employs',\n",
       " 'enable',\n",
       " 'endured',\n",
       " 'enoch',\n",
       " 'errors',\n",
       " 'erupts',\n",
       " 'eu',\n",
       " 'euros',\n",
       " 'evan',\n",
       " 'evokes',\n",
       " 'execute',\n",
       " 'exhibit',\n",
       " 'exiled',\n",
       " 'exit',\n",
       " 'extremes',\n",
       " 'facilities',\n",
       " 'factions',\n",
       " 'females',\n",
       " 'financing',\n",
       " 'fixed',\n",
       " 'flirts',\n",
       " 'foreman',\n",
       " 'francesca',\n",
       " 'frankie',\n",
       " 'frederick',\n",
       " 'freed',\n",
       " 'freshman',\n",
       " 'frightened',\n",
       " 'fulfilling',\n",
       " 'gerry',\n",
       " 'gibson',\n",
       " 'goddess',\n",
       " 'godfather',\n",
       " 'grades',\n",
       " 'greenland',\n",
       " 'grieving',\n",
       " 'grocery',\n",
       " 'groom',\n",
       " 'grownup',\n",
       " 'gruesome',\n",
       " 'guatemala',\n",
       " 'haasan',\n",
       " 'haji',\n",
       " 'hamburg',\n",
       " 'harvard',\n",
       " 'hawaiian',\n",
       " 'heartbroken',\n",
       " 'helena',\n",
       " 'herd',\n",
       " 'heroism',\n",
       " 'highs',\n",
       " 'horn',\n",
       " 'horrified',\n",
       " 'hurricane',\n",
       " 'imminent',\n",
       " 'industries',\n",
       " 'inexperienced',\n",
       " 'insistence',\n",
       " 'installment',\n",
       " 'instructions',\n",
       " 'interviewing',\n",
       " 'jamal',\n",
       " 'jeanpierre',\n",
       " 'johannes',\n",
       " 'junk',\n",
       " 'kane',\n",
       " 'karin',\n",
       " 'katherine',\n",
       " 'kathleen',\n",
       " 'kathy',\n",
       " 'kenya',\n",
       " 'kilometres',\n",
       " 'kostas',\n",
       " 'kurt',\n",
       " 'lance',\n",
       " 'leap',\n",
       " 'lecture',\n",
       " 'legally',\n",
       " 'lighthearted',\n",
       " 'liquor',\n",
       " 'load',\n",
       " 'loans',\n",
       " 'logic',\n",
       " 'lords',\n",
       " 'lotus',\n",
       " 'luc',\n",
       " 'luna',\n",
       " 'lured',\n",
       " 'lush',\n",
       " 'machinery',\n",
       " 'madam',\n",
       " 'mainland',\n",
       " 'managers',\n",
       " 'manchester',\n",
       " 'manipulate',\n",
       " 'manipulation',\n",
       " 'marginalized',\n",
       " 'marias',\n",
       " 'marker',\n",
       " 'meals',\n",
       " 'meaningful',\n",
       " 'mecca',\n",
       " 'meteoric',\n",
       " 'meticulously',\n",
       " 'mgm',\n",
       " 'migrants',\n",
       " 'ming',\n",
       " 'miniseries',\n",
       " 'minority',\n",
       " 'mobster',\n",
       " 'montage',\n",
       " 'mormon',\n",
       " 'mortgage',\n",
       " 'mosaic',\n",
       " 'muslims',\n",
       " 'myriad',\n",
       " 'namely',\n",
       " 'natives',\n",
       " 'neighborhoods',\n",
       " 'nerds',\n",
       " 'nerves',\n",
       " 'nightclubs',\n",
       " 'nonetheless',\n",
       " 'nu',\n",
       " 'nutrition',\n",
       " 'obliged',\n",
       " 'offensive',\n",
       " 'offspring',\n",
       " 'oldfashioned',\n",
       " 'outraged',\n",
       " 'pages',\n",
       " 'painters',\n",
       " 'parody',\n",
       " 'participated',\n",
       " 'patron',\n",
       " 'peninsula',\n",
       " 'perceptions',\n",
       " 'phenomenal',\n",
       " 'pickup',\n",
       " 'placing',\n",
       " 'plate',\n",
       " 'playground',\n",
       " 'poker',\n",
       " 'polly',\n",
       " 'poorest',\n",
       " 'posed',\n",
       " 'posted',\n",
       " 'potentially',\n",
       " 'profoundly',\n",
       " 'promoted',\n",
       " 'promoting',\n",
       " 'proudly',\n",
       " 'puppet',\n",
       " 'purity',\n",
       " 'puzzle',\n",
       " 'quarterback',\n",
       " 'quarters',\n",
       " 'queer',\n",
       " 'racers',\n",
       " 'rainbow',\n",
       " 'reasonable',\n",
       " 'rediscover',\n",
       " 'reed',\n",
       " 'reefs',\n",
       " 'reflected',\n",
       " 'reflecting',\n",
       " 'refusal',\n",
       " 'regards',\n",
       " 'reminded',\n",
       " 'requests',\n",
       " 'reservation',\n",
       " 'resolves',\n",
       " 'retarded',\n",
       " 'revived',\n",
       " 'richards',\n",
       " 'rigid',\n",
       " 'rivalries',\n",
       " 'robber',\n",
       " 'rowdy',\n",
       " 'ruling',\n",
       " 'saiful',\n",
       " 'salim',\n",
       " 'sanjay',\n",
       " 'sasaki',\n",
       " 'screens',\n",
       " 'seller',\n",
       " 'senator',\n",
       " 'senseless',\n",
       " 'sentimental',\n",
       " 'serial',\n",
       " 'shapes',\n",
       " 'sharon',\n",
       " 'sheltered',\n",
       " 'shelters',\n",
       " 'shifting',\n",
       " 'sickness',\n",
       " 'sincerity',\n",
       " 'sins',\n",
       " 'skies',\n",
       " 'smiling',\n",
       " 'snapshot',\n",
       " 'snowy',\n",
       " 'soup',\n",
       " 'specially',\n",
       " 'spirals',\n",
       " 'spouses',\n",
       " 'stills',\n",
       " 'streak',\n",
       " 'subjective',\n",
       " 'sunshine',\n",
       " 'supervisor',\n",
       " 'swim',\n",
       " 'syrian',\n",
       " 'tail',\n",
       " 'takashi',\n",
       " 'takeshi',\n",
       " 'tame',\n",
       " 'targeted',\n",
       " 'targets',\n",
       " 'testify',\n",
       " 'texts',\n",
       " 'theresa',\n",
       " 'thesis',\n",
       " 'thug',\n",
       " 'timothy',\n",
       " 'tobacco',\n",
       " 'tolerance',\n",
       " 'towering',\n",
       " 'transgender',\n",
       " 'transsexual',\n",
       " 'trois',\n",
       " 'trunk',\n",
       " 'tyson',\n",
       " 'unforgiving',\n",
       " 'unorthodox',\n",
       " 'unsuccessfully',\n",
       " 'unveils',\n",
       " 'vanity',\n",
       " 'verdict',\n",
       " 'versus',\n",
       " 'wellbeing',\n",
       " 'weve',\n",
       " 'wheels',\n",
       " 'whites',\n",
       " 'withdrawn',\n",
       " 'workaholic',\n",
       " 'wrath',\n",
       " 'ww2',\n",
       " 'yu',\n",
       " 'от',\n",
       " 'abel',\n",
       " 'abu',\n",
       " 'abundance',\n",
       " 'accompanying',\n",
       " 'accomplice',\n",
       " 'acknowledge',\n",
       " 'acted',\n",
       " 'ada',\n",
       " 'adultery',\n",
       " 'agony',\n",
       " 'ahmed',\n",
       " 'aimed',\n",
       " 'aki',\n",
       " 'alienated',\n",
       " 'allamerican',\n",
       " 'ally',\n",
       " 'alma',\n",
       " 'andrews',\n",
       " 'angola',\n",
       " 'anthology',\n",
       " 'apocalypse',\n",
       " 'arduous',\n",
       " 'ashley',\n",
       " 'aspires',\n",
       " 'assets',\n",
       " 'assisted',\n",
       " 'atlanta',\n",
       " 'aya',\n",
       " 'ayumi',\n",
       " 'bags',\n",
       " 'bailey',\n",
       " 'bakery',\n",
       " 'balkan',\n",
       " 'baltimore',\n",
       " 'barrel',\n",
       " 'barriers',\n",
       " 'bearing',\n",
       " 'beethoven',\n",
       " 'billions',\n",
       " 'bitterness',\n",
       " 'blacks',\n",
       " 'bliss',\n",
       " 'bollywood',\n",
       " 'bon',\n",
       " 'bonnie',\n",
       " 'boyhood',\n",
       " 'brash',\n",
       " 'breed',\n",
       " 'bret',\n",
       " 'brightest',\n",
       " 'brilliance',\n",
       " 'brilliantly',\n",
       " 'burn',\n",
       " 'businesses',\n",
       " 'cake',\n",
       " 'calendar',\n",
       " 'canadas',\n",
       " 'capitol',\n",
       " 'caracas',\n",
       " 'carroll',\n",
       " 'cart',\n",
       " 'castro',\n",
       " 'cathy',\n",
       " 'cest',\n",
       " 'checks',\n",
       " ...]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dtm = movies['Genre 1'].reset_index()[movies['Genre 1'].reset_index().columns.difference(['index'])]\n",
    "# groups_filter = pd.concat([dtm, df], axis=1)\n",
    "# groups_filter = groups_filter.groupby('Genre 1').sum()[df.columns]\n",
    "# groups_filter = groups_filter.T\n",
    "# groups_filter['n_words'] = groups_filter.apply(sum, axis = 1)\n",
    "\n",
    "# # Taking into consideration n_words which is always > 0\n",
    "# groups_filter['is_common'] = groups_filter.apply(lambda x: 1 if sum(x > 0) > 2 else 0, axis = 1)\n",
    "\n",
    "# groups_filter.sort_values(['is_common', 'n_words'], ascending = False)\n",
    "\n",
    "# # Get rid of top 20% words which are common (?)\n",
    "# groups_filter = groups_filter.sort_values(['is_common', 'n_words'], ascending = False).iloc[int(np.floor(groups_filter.shape[0]/5)):groups_filter.shape[0]]\n",
    "\n",
    "# word_list = list(groups_filter.index)\n",
    "# word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Genre 1\n",
       "Comedy         15700\n",
       "Documentary    19137\n",
       "Drama          19332\n",
       "dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groups_filter.iloc[:, 0:groups_filter.shape[1]-2]\n",
    "# groups_filter = groups_filter.iloc[:, 0:groups_filter.shape[1]-2]\n",
    "# n_filter = (groups_filter > 0).sum()\n",
    "# n_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Genre 1</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>Drama</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abbey</th>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abel</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abraham</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accent</th>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acquainted</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acted</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adjust</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adultery</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adversity</th>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advises</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advocates</th>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>afghan</th>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agatha</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aggression</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ajith</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alas</th>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alekos</th>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alexis</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alienated</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ally</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alma</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alt</th>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alzheimers</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amira</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anand</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ang</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>angle</th>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>angst</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoying</th>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>appreciation</th>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wallet</th>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wanda</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weakness</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>welcomes</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whale</th>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whereabouts</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whore</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wildebeest</th>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>willy</th>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>withdrawn</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>workaholic</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worldly</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wound</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wrath</th>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wrestlers</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yakuza</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yankees</th>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yorks</th>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yu</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zana</th>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zen</th>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zu</th>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>är</th>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>år</th>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>του</th>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>все</th>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>за</th>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>как</th>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>от</th>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>это</th>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Genre 1     Comedy  Documentary  Drama\n",
       "abbey          NaN         15.0    NaN\n",
       "abel           NaN          NaN   13.0\n",
       "abraham        NaN          NaN   14.0\n",
       "accent         9.0          NaN    NaN\n",
       "acquainted     NaN          NaN   11.0\n",
       "...            ...          ...    ...\n",
       "все           10.0          NaN    NaN\n",
       "за            11.0          NaN    NaN\n",
       "как            9.0          NaN    NaN\n",
       "от            12.0          NaN    NaN\n",
       "это           12.0          NaN    NaN\n",
       "\n",
       "[600 rows x 3 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groups_filter = groups_filter.T\n",
    "\n",
    "# def sorted(s, num):\n",
    "#     tmp = s.sort_values(ascending=False)[:num]  # earlier s.order(..)\n",
    "# #     tmp.index = range(num)\n",
    "#     return tmp\n",
    "\n",
    "# groups_filter = groups_filter[word_list]\n",
    "# groups_filter\n",
    "# sorted_words = groups_filter.T.apply(lambda x: sorted(x, 200))\n",
    "# sorted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Each row now has 4 intersections with all 4 genres\n",
    "# df_array = df_test[list(sorted_words.index)].values\n",
    "# tfm = [row * (~sorted_words.isnull().T) for row in df_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comedy_base = ~sorted_words['Comedy'][~sorted_words.isnull()['Comedy'].T].isnull()\n",
    "# # horror_base = ~sorted_words['Horror'][~sorted_words.isnull()['Horror'].T].isnull()\n",
    "# documentary_base = ~sorted_words['Documentary'][~sorted_words.isnull()['Documentary'].T].isnull()\n",
    "# drama_base = ~sorted_words['Drama'][~sorted_words.isnull()['Drama'].T].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stooge      1.000000\n",
       "det         0.983607\n",
       "que         0.968254\n",
       "standup     0.943182\n",
       "til         0.936170\n",
       "              ...   \n",
       "teach       0.378947\n",
       "learns      0.377049\n",
       "star        0.376884\n",
       "another     0.376068\n",
       "everyone    0.375000\n",
       "Name: Comedy, Length: 200, dtype: float64"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comedy_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overview</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level_1</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>The pert Tina is sick of school and the muff i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>She wants to go with Tino - attendant of a vis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>However he lets her down and leaves alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>Now Tina persuades fellow student Robby, who h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>It seems only to be a matter of time until he ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2030</th>\n",
       "      <td>Frankenstein Jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2030</th>\n",
       "      <td>is frustrated at his inability to date success...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2030</th>\n",
       "      <td>He seeks out his friend Dracula Jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2030</th>\n",
       "      <td>for dating advice.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2057</th>\n",
       "      <td>Herr Steinkoehler is a passionate pedestrian, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2057</th>\n",
       "      <td>One of them Steinkoehler bought out of pity fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2057</th>\n",
       "      <td>Both of them enroll in a driving school</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2057</th>\n",
       "      <td>For Steinkoehler, the lessons, or more precise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2057</th>\n",
       "      <td>Hempel exercises his powers to their fullest e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2057</th>\n",
       "      <td>In addition, Steinkoehler becomes jealous when...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2057</th>\n",
       "      <td>Yet fate would have it that a flighty, young, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2057</th>\n",
       "      <td>Her devotion both lifts and confuses him with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>Two friends are invited for a weekend to a lux...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>The boss gets shot and nobody seems to notice,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>In order not to become suspects of murder they...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>The killer wants to do his job so when he is i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>Guillaume has made it: A machine that can clea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>Some Japanese business guys, after dinner with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>His only problem: His production capacity is w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>His wife Bernadette is far from being happy ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>Her private life goes down the line so she dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2110</th>\n",
       "      <td>In a village of the Po valley where the earth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2110</th>\n",
       "      <td>If in secret, they admired and liked each othe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2110</th>\n",
       "      <td>And when the mayor wants his \"People's House\";...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2110</th>\n",
       "      <td>Division exist between the richest and the poo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17850</th>\n",
       "      <td>As the law and order are destroyed, the world ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17850</th>\n",
       "      <td>Chiu-Meng, a military officer, has been lookin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17850</th>\n",
       "      <td>Driven by desperation, she turns herself into ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17850</th>\n",
       "      <td>Only when she runs into a man and a boy who in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17850</th>\n",
       "      <td>Nevertheless, they soon get trapped by a group...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17850</th>\n",
       "      <td>Chiu-Meng now has to fight to protect this dim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17871</th>\n",
       "      <td>After taking a break for the summer before the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17871</th>\n",
       "      <td>When Ryan's mother demands a paternity test, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17871</th>\n",
       "      <td>The other father is Ryan's best friend Bryce, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17871</th>\n",
       "      <td>With college recruiters and an overbearing fat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17871</th>\n",
       "      <td>(2017)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17891</th>\n",
       "      <td>Robert Eastwood is an innocent, brought up in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17891</th>\n",
       "      <td>When he goes to school he's shocked to discove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17891</th>\n",
       "      <td>Caught between his Mother, who's determined to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17891</th>\n",
       "      <td>It's then that he discovers a dead body in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17891</th>\n",
       "      <td>The Devil Outside is a story of everyday madness.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17892</th>\n",
       "      <td>YeJoo and EunMin spent two years as a couple s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17892</th>\n",
       "      <td>One day, YeJoo wants to finish the relationshi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17892</th>\n",
       "      <td>EunMin gets angry and asks if there's another ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17892</th>\n",
       "      <td>Yejoo says she got a boyfriend.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18041</th>\n",
       "      <td>It’s another ordinary night for Deno and Sara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18041</th>\n",
       "      <td>Their father is drunk as usual, and the two of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18041</th>\n",
       "      <td>But in this instance, straying from routine b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18041</th>\n",
       "      <td>Ambi is a short movie about the adult in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18041</th>\n",
       "      <td>It is a movie about changing roles, the darkes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18041</th>\n",
       "      <td>But it is also a movie about humanity and love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18064</th>\n",
       "      <td>The film follows Paula, a young art student, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18064</th>\n",
       "      <td>Pain-struck, she runs up against the ignorance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18064</th>\n",
       "      <td>Paula and her family realize that they are jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18064</th>\n",
       "      <td>Driven by anger, frustration and sorrow, she c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3259 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  overview\n",
       "level_1                                                   \n",
       "2024     The pert Tina is sick of school and the muff i...\n",
       "2024     She wants to go with Tino - attendant of a vis...\n",
       "2024             However he lets her down and leaves alone\n",
       "2024     Now Tina persuades fellow student Robby, who h...\n",
       "2024     It seems only to be a matter of time until he ...\n",
       "...                                                    ...\n",
       "18041    But it is also a movie about humanity and love...\n",
       "18064    The film follows Paula, a young art student, t...\n",
       "18064    Pain-struck, she runs up against the ignorance...\n",
       "18064    Paula and her family realize that they are jus...\n",
       "18064    Driven by anger, frustration and sorrow, she c...\n",
       "\n",
       "[3259 rows x 1 columns]"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level_1\n",
       "2024     The pert Tina is sick of school and the muff i...\n",
       "2024     She wants to go with Tino - attendant of a vis...\n",
       "2024             However he lets her down and leaves alone\n",
       "2024     Now Tina persuades fellow student Robby, who h...\n",
       "2024     It seems only to be a matter of time until he ...\n",
       "                               ...                        \n",
       "18041    But it is also a movie about humanity and love...\n",
       "18064    The film follows Paula, a young art student, t...\n",
       "18064    Pain-struck, she runs up against the ignorance...\n",
       "18064    Paula and her family realize that they are jus...\n",
       "18064    Driven by anger, frustration and sorrow, she c...\n",
       "Name: overview, Length: 3259, dtype: object"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences['overview']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['stooge', 'det', 'que', 'standup', 'til', 'comedy', 'og', 'il',\n",
       "       'på', 'har', 'se', 'den', 'hilarious', 'een', 'comedian', 'het',\n",
       "       'med', 'con', 'en', 'er', 'som', 'et', 'laugh', 'han', 'sexy',\n",
       "       'crazy', 'un', 'di', 'le', 'accidentally', 'de', 'uncle',\n",
       "       'christmas', 'mr', 'pretend', 'agrees', 'hot', 'da', 'funny',\n",
       "       'der', 'isnt', 'holiday', 'sure', 'bos', 'pretty', 'plan',\n",
       "       'appearance', 'theatre', 'dvd', 'tv', 'guy', 'unfortunately',\n",
       "       'bit', 'charlie', 'wedding', 'lady', 'comic', 'gang', 'guest',\n",
       "       'romantic', 'fun', 'dont', 'humor', 'expect', 'miss', 'girlfriend',\n",
       "       'quite', 'manager', 'trouble', 'hotel', 'buy', 'van', 'audience',\n",
       "       'dress', 'charm', 'hire', 'george', 'la', 'rich', 'bad', 'office',\n",
       "       'bus', 'date', 'host', 'successful', 'boyfriend', 'decide',\n",
       "       'invite', 'realize', 'party', 'happy', 'manage', 'show', 'win',\n",
       "       'steal', 'cant', 'owner', 'hit', 'get', 'divorce', 'fail', 'money',\n",
       "       'think', 'marry', 'doesnt', 'decides', 'store', 'special',\n",
       "       'arrives', 'sell', 'married', 'actually', 'best', 'pay', 'met',\n",
       "       'especially', 'perform', 'happens', 'lot', 'meanwhile', 'job',\n",
       "       'radio', 'plot', 'wealthy', 'caught', 'problem', 'bill', 'tour',\n",
       "       'finally', 'wrong', 'wife', 'need', 'thing', 'company', 'stand',\n",
       "       'spend', 'car', 'quickly', 'shop', 'baby', 'couple', 'however',\n",
       "       'door', 'always', 'stop', 'good', 'joe', 'order', 'perfect',\n",
       "       'suddenly', 'try', 'appear', 'really', 'big', 'television',\n",
       "       'festival', 'brother', 'shes', 'someone', 'house', 'involve',\n",
       "       'arrive', 'husband', 'nothing', 'king', 'dead', 'turn', 'friend',\n",
       "       'short', 'fire', 'stage', 'girl', 'officer', 'room', 'situation',\n",
       "       'actress', 'able', 'letter', 'rather', 'want', 'soon', 'keep',\n",
       "       'singer', 'let', 'seem', 'teacher', 'single', 'course', 'trip',\n",
       "       'sent', 'put', 'idea', 'stay', 'enough', 'popular', 'teach',\n",
       "       'learns', 'star', 'another', 'everyone'], dtype=object)"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comedy_top_words.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boyfriend',\n",
       " 'manage',\n",
       " 'seem',\n",
       " 'let',\n",
       " 'win',\n",
       " 'trip',\n",
       " 'want',\n",
       " 'however',\n",
       " 'shes']"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection(mj_dtm(sentences[:5]['overview']).columns.values, comedy_top_words.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mj_class(x, base):\n",
    "    res = mj_dtm(x)\n",
    "    res = res[intersection(res.columns.values, base.index.values)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>boyfriend</th>\n",
       "      <th>manage</th>\n",
       "      <th>seem</th>\n",
       "      <th>let</th>\n",
       "      <th>win</th>\n",
       "      <th>trip</th>\n",
       "      <th>want</th>\n",
       "      <th>however</th>\n",
       "      <th>shes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level_1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         boyfriend  manage  seem  let  win  trip  want  however  shes\n",
       "level_1                                                              \n",
       "2024             0       0     0    0    0     0     0        0     0\n",
       "2024             0       0     0    0    0     0     1        0     0\n",
       "2024             0       0     0    1    0     0     0        1     0\n",
       "2024             0       0     0    0    0     1     0        0     0\n",
       "2024             1       1     1    0    1     0     0        0     1"
      ]
     },
     "execution_count": 621,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:5].groupby('level_1').apply(lambda x: mj_class(x['overview'], comedy_top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['van', 'het', 'con', 'agrees'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-574-5a5d03a6f828>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcomedy_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcomedy_top_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# horror_test = df_test[horror_base.index.values]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdocumentary_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdocumentary_top_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdrama_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdrama_top_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2979\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2980\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2981\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2983\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[1;32m   1269\u001b[0m                 \u001b[0;31m# When setting, missing keys are not allowed, even with .loc:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"raise_missing\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_setter\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1271\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1272\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m         self._validate_read_indexer(\n\u001b[0;32m-> 1078\u001b[0;31m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1079\u001b[0m         )\n\u001b[1;32m   1080\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1169\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"loc\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} not in index\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnot_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m             \u001b[0;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['van', 'het', 'con', 'agrees'] not in index\""
     ]
    }
   ],
   "source": [
    "comedy_test = df_test[comedy_top_words.index.values]\n",
    "# horror_test = df_test[horror_base.index.values]\n",
    "documentary_test = df_test[documentary_top_words.index.values]\n",
    "drama_test = df_test[drama_top_words.index.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def calculate_dist_matrix(x):\n",
    "    dist_matrix = np.arccos(np.round(cosine_similarity(x), 5))\n",
    "    aux_zeros = np.zeros(dist_matrix.shape[0] - 1)\n",
    "    np.fill_diagonal(dist_matrix[1:], aux_zeros)\n",
    "    np.fill_diagonal(dist_matrix[:,1:], aux_zeros)\n",
    "    np.fill_diagonal(dist_matrix, 0)\n",
    "    return dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "comedy_test_dist = pd.DataFrame(comedy_test.groupby(comedy_test.index).apply(lambda x: calculate_dist_matrix(x)))\n",
    "documentary_test_dist = pd.DataFrame(documentary_test.groupby(documentary_test.index).apply(lambda x: calculate_dist_matrix(x)))\n",
    "drama_test_dist = pd.DataFrame(drama_test.groupby(drama_test.index).apply(lambda x: calculate_dist_matrix(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.78539361, 1.57079633],\n",
       "       [0.        , 0.        , 0.        , 1.57079633],\n",
       "       [0.78539361, 0.        , 0.        , 0.        ],\n",
       "       [1.57079633, 1.57079633, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comedy_test_dist.loc[2017][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 1.57079633, 1.57079633, 1.57079633],\n",
       "       [0.        , 0.        , 0.        , 0.78539361, 1.57079633],\n",
       "       [1.57079633, 0.        , 0.        , 0.        , 1.57079633],\n",
       "       [1.57079633, 0.78539361, 0.        , 0.        , 0.        ],\n",
       "       [1.57079633, 1.57079633, 1.57079633, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comedy_test_dist.loc[2020][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <span>StrVector with 9 elements.</span>\n",
       "        <table>\n",
       "        <tbody>\n",
       "          <tr>\n",
       "          \n",
       "            <td>\n",
       "            'TDA'\n",
       "            </td>\n",
       "          \n",
       "            <td>\n",
       "            'tools'\n",
       "            </td>\n",
       "          \n",
       "            <td>\n",
       "            'stats'\n",
       "            </td>\n",
       "          \n",
       "            <td>\n",
       "            ...\n",
       "            </td>\n",
       "          \n",
       "            <td>\n",
       "            'datasets'\n",
       "            </td>\n",
       "          \n",
       "            <td>\n",
       "            'methods'\n",
       "            </td>\n",
       "          \n",
       "            <td>\n",
       "            'base'\n",
       "            </td>\n",
       "          \n",
       "          </tr>\n",
       "        </tbody>\n",
       "        </table>\n",
       "        "
      ],
      "text/plain": [
       "R object with classes: ('character',) mapped to:\n",
       "['TDA', 'tools', 'stats', 'graphics', ..., 'utils', 'datasets', 'methods', 'base']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy as sp\n",
    "from rpy2.robjects.packages import importr\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import r\n",
    "# import pandas.rpy.common as com\n",
    "\n",
    "# import R's \"base\" package\n",
    "base = importr('base')\n",
    "\n",
    "# import R's \"utils\" package\n",
    "utils = importr('utils')\n",
    "\n",
    "# import rpy2's package module\n",
    "import rpy2.robjects.packages as rpackages\n",
    "\n",
    "# import R's utility package\n",
    "utils = rpackages.importr('utils')\n",
    "\n",
    "# select a mirror for R packages\n",
    "utils.chooseCRANmirror(ind=1) # select the first mirror in the list\n",
    "\n",
    "# R package names\n",
    "packnames = ('TDA')\n",
    "\n",
    "# R vector of strings\n",
    "from rpy2.robjects.vectors import StrVector\n",
    "\n",
    "utils.install_packages(StrVector('TDA'))\n",
    "r('library(TDA)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2\n",
    "from rpy2.robjects import pandas2ri # install any dependency package if you get error like \"module not found\"\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "from rpy2.robjects import globalenv\n",
    "\n",
    "def r_convert(x):\n",
    "    pandas2ri.activate()\n",
    "    \n",
    "    aux_tmp = pd.DataFrame(pd.Series(x.values.tolist()).loc[0][0])\n",
    "    \n",
    "    with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "        r_df = ro.conversion.py2rpy(aux_tmp)\n",
    "        \n",
    "    globalenv['r_df'] = r_df\n",
    "    r('Diag <- ripsDiag(X = r_df, 1, max(r_df), library = \"Dionysus\", dist = \"arbitrary\", printProgress = FALSE)')\n",
    "    return(r('Diag$diagram'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comedy_rips = comedy_test_dist.groupby(comedy_test_dist.index).apply(lambda x: r_convert(x))\n",
    "# horror_rips = horror_test.groupby(horror_test.index).apply(lambda x: r_convert(x))\n",
    "documentary_rips = documentary_test_dist.groupby(documentary_test_dist.index).apply(lambda x: r_convert(x))\n",
    "drama_rips = drama_test_dist.groupby(drama_test_dist.index).apply(lambda x: r_convert(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1153169014084507"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comedy_rips.apply(lambda x: np.any(x[:, 0] == 1)).values.mean()\n",
    "documentary_rips.apply(lambda x: np.any(x[:, 0] == 1)).values.mean()\n",
    "drama_rips.apply(lambda x: np.any(x[:, 0] == 1)).values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Drama          746\n",
       "Comedy         626\n",
       "Documentary    463\n",
       "Name: Genre 1, dtype: int64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.loc[comedy_rips[comedy_rips.apply(lambda x: np.any(x[:, 0] == 1)).values].index.values]['Genre 1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3411444141689373"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(movies.loc[comedy_rips[comedy_rips.apply(lambda x: np.any(x[:, 0] == 1)).values].index.values]['Genre 1'] == 'Comedy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6551528878822197"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(movies.loc[documentary_rips[documentary_rips.apply(lambda x: np.any(x[:, 0] == 1)).values].index.values]['Genre 1'] == 'Documentary').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48909487459105777"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(movies.loc[drama_rips[drama_rips.apply(lambda x: np.any(x[:, 0] == 1)).values].index.values]['Genre 1'] == 'Drama').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: dev.new(): using pdf(file=\"Rplots1.pdf\")\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int32)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r('dev.new()') # optional: create a new figure\n",
    "r('plot(Diag$diagram, barcode=TRUE)')\n",
    "r('dev.off()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df = df.iloc[:, (groups_filter.apply(lambda x: sum(x != 0)) == 1).values]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "tfidf = transformer.fit_transform(df)\n",
    "tfidf = pd.DataFrame(tfidf.toarray())\n",
    "tfidf.columns = df.columns.difference(['Genre 1'])\n",
    "tfidf\n",
    "\n",
    "tfidf = tfidf.reset_index()[tfidf.reset_index().columns.difference(['index'])]\n",
    "first_class = pd.concat([dtm, tfidf], axis=1)\n",
    "first_class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
